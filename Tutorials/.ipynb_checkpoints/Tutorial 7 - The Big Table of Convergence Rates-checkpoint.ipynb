{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the past 50+ years of convex optimization research, a great many\n",
    "algorithms have been developed, each with slight nuances to their assumptions,\n",
    "implementations, and guarantees. In this article, I'll give a shorthand\n",
    "comparison of these methods in terms of the number of iterations required\n",
    "to reach a desired accuracy <mathjax>$\\epsilon$</mathjax> for convex and strongly convex objective\n",
    "functions.</p>\n",
    "<p>Below, methods are grouped according to what \"order\" of information they\n",
    "require about the objective function. In general, the more information you\n",
    "have, the faster you can converge; but beware, you will also need more memory\n",
    "and computation. Zeroth and first order methods are typically appropriate for\n",
    "large scale problems, whereas second order methods are limited to\n",
    "small-to-medium scale problems that require a high degree of precision.</p>\n",
    "<p>At the bottom, you will find algorithms aimed specifically at minimizing\n",
    "supervised learning problems and other meta-algorithms useful for distributing\n",
    "computation across multiple nodes.</p>\n",
    "<p>Unless otherwise stated, all objectives are assumed to be Lipschitz\n",
    "continuous (though not necssarily differentiable) and the domain convex. The\n",
    "variable being optimized is <mathjax>$x \\in \\mathbb{R}^n$</mathjax>.</p>\n",
    "<h1>Zero-th Order Methods</h1>\n",
    "<p>Zeroth order methods are characterized by not requiring any gradients or\n",
    "subgradients for their objective functions. In exchange, however, it is\n",
    "assumed that the objective is \"simple\" in the sense that a subset of variables\n",
    "(a \"block\") can be minimized exactly while holding all other variables fixed.</p>\n",
    "<table class=\"table table-bordered table-centered\">\n",
    "<p><colgroup>\n",
    "    <col style=\"width:20%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:40%\">\n",
    "  </colgroup>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Algorithm          </th>\n",
    "      <th>Problem Formulation</th>\n",
    "      <th>Convex             </th>\n",
    "      <th>Strongly Convex    </th>\n",
    "      <th>Per-Iteration Cost </th>\n",
    "      <th>Notes              </th>\n",
    "    </tr>\n",
    "  </thead></p>\n",
    "<p><tbody>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Randomized Block Coordinate Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^{n}} f(x) + g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon)$</mathjax><sup id=\"fnref:richtarik-2011\"><a class=\"footnote-ref\" href=\"#fn:richtarik-2011\" rel=\"footnote\">8</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:richtarik-2011\"><a class=\"footnote-ref\" href=\"#fn:richtarik-2011\" rel=\"footnote\">8</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(1)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f(x)$</mathjax> is differentiable and <mathjax>$g(x)$</mathjax> is separable in\n",
    "        each block. <mathjax>$g(x)$</mathjax> may be a barrier function.\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody></p>\n",
    "</table>\n",
    "<h1>First Order Methods</h1>\n",
    "<p>First order methods typically require access to an objective function's\n",
    "gradient or subgradient. The algorithms typically take the form <mathjax>$x^{(t+1)}\n",
    "= x^{(t)} - \\alpha^{(t)} g^{(t)}$</mathjax> for some step size <mathjax>$\\alpha^{(t)}$</mathjax> and descent\n",
    "direction <mathjax>$g^{(t)}$</mathjax>. As such, each iteration takes approximately <mathjax>$O(n)$</mathjax> time.</p>\n",
    "<table class=\"table table-bordered table-centered\">\n",
    "<p><colgroup>\n",
    "    <col style=\"width:20%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:40%\">\n",
    "  </colgroup></p>\n",
    "<p><thead>\n",
    "    <tr>\n",
    "      <th>Algorithm          </th>\n",
    "      <th>Problem Formulation</th>\n",
    "      <th>Convex             </th>\n",
    "      <th>Strongly Convex    </th>\n",
    "      <th>Per-Iteration Cost </th>\n",
    "      <th>Notes              </th>\n",
    "    </tr>\n",
    "  </thead></p>\n",
    "<p><tbody>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Subgradient Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle  \\min_{x \\in \\mathbb{R}^n} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon^{2})$</mathjax><sup id=\"fnref:blog-sd\"><a class=\"footnote-ref\" href=\"#fn:blog-sd\" rel=\"footnote\">2</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td>...</td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Cannot be improved upon without further assumptions.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Mirror Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathcal{C}} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon^{2} )$</mathjax><sup id=\"fnref:ee381-md\"><a class=\"footnote-ref\" href=\"#fn:ee381-md\" rel=\"footnote\">9</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(1 / \\epsilon )$</mathjax><sup id=\"fnref:nedich-2013\"><a class=\"footnote-ref\" href=\"#fn:nedich-2013\" rel=\"footnote\">26</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Different parameterizations result in gradient descent and\n",
    "        exponentiated gradient descent.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Dual Averaging</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle  \\min_{x \\in \\mathcal{C}} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon^{2})$</mathjax><sup id=\"fnref:nesterov-2007\"><a class=\"footnote-ref\" href=\"#fn:nesterov-2007\" rel=\"footnote\">25</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td>...</td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Cannot be improved upon without further assumptions.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Gradient Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon)$</mathjax><sup id=\"fnref:blog-gd\"><a class=\"footnote-ref\" href=\"#fn:blog-gd\" rel=\"footnote\">1</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:ee381-gd\"><a class=\"footnote-ref\" href=\"#fn:ee381-gd\" rel=\"footnote\">10</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f(x)$</mathjax> is differentiable.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Accelerated Gradient Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\sqrt{\\epsilon})$</mathjax><sup id=\"fnref:blog-agd\"><a class=\"footnote-ref\" href=\"#fn:blog-agd\" rel=\"footnote\">3</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:bubeck-agd\"><a class=\"footnote-ref\" href=\"#fn:bubeck-agd\" rel=\"footnote\">11</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f(x)$</mathjax> is differentiable.\n",
    "        Cannot be improved upon without further assumptions.\n",
    "        Has better constants than Gradient Descent for \"Strongly Convex\" case.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Proximal Gradient Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathcal{C}} f(x) + g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon)$</mathjax><sup id=\"fnref:blog-pgd\"><a class=\"footnote-ref\" href=\"#fn:blog-pgd\" rel=\"footnote\">4</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:mairal-2013\"><a class=\"footnote-ref\" href=\"#fn:mairal-2013\" rel=\"footnote\">13</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f(x)$</mathjax> is differentiable and\n",
    "        <mathjax>$\\text{prox}_{\\tau_t g}(x)$</mathjax> is easily computable.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Proximal Accelerated Gradient Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathcal{C}} f(x) + g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\sqrt{\\epsilon})$</mathjax><sup id=\"fnref:blog-apgd\"><a class=\"footnote-ref\" href=\"#fn:blog-apgd\" rel=\"footnote\">5</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:mairal-2013\"><a class=\"footnote-ref\" href=\"#fn:mairal-2013\" rel=\"footnote\">13</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f(x)$</mathjax> is differentiable and\n",
    "        <mathjax>$\\text{prox}_{\\tau_t g}(x)$</mathjax> is easily computable.\n",
    "        Has better constants than Proximal Gradient Descent for \"Strongly\n",
    "        Convex\" case.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Frank-Wolfe Algorithm / Conditional Gradient Algorithm</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathcal{C}} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1/\\epsilon)$</mathjax><sup id=\"fnref:blog-fw\"><a class=\"footnote-ref\" href=\"#fn:blog-fw\" rel=\"footnote\">6</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(1/\\sqrt{\\epsilon})$</mathjax><sup id=\"fnref:garber-2014\"><a class=\"footnote-ref\" href=\"#fn:garber-2014\" rel=\"footnote\">12</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$\\mathcal{C}$</mathjax> is bounded and <mathjax>$h_{g}(x) = \\arg\\min_{x \\in\n",
    "        \\mathcal{C}} \\langle g, x \\rangle$</mathjax> is easily computable. Most useful\n",
    "        when <mathjax>$\\mathcal{C}$</mathjax> is a polytope in a very high dimensional space with\n",
    "        sparse extrema.\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody></p>\n",
    "</table>\n",
    "<h1>Second Order Methods</h1>\n",
    "<p>Second order methods either use or approximate the hessian (<mathjax>$\\nabla^2 f(x)$</mathjax>)\n",
    "of the objective function to result in better-than-linear rates of convergence.\n",
    "As such, each iteration typically requires <mathjax>$O(n^2)$</mathjax> memory and between <mathjax>$O(n^2)$</mathjax>\n",
    "and <mathjax>$O(n^3)$</mathjax> computation per iteration.</p>\n",
    "<table class=\"table table-bordered table-centered\">\n",
    "<p><colgroup>\n",
    "    <col style=\"width:20%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:40%\">\n",
    "  </colgroup></p>\n",
    "<p><thead>\n",
    "    <tr>\n",
    "      <th>Algorithm          </th>\n",
    "      <th>Problem Formulation</th>\n",
    "      <th>Convex             </th>\n",
    "      <th>Strongly Convex    </th>\n",
    "      <th>Per-Iteration Cost </th>\n",
    "      <th>Notes              </th>\n",
    "    </tr>\n",
    "  </thead></p>\n",
    "<p><tbody>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Newton's Method</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td>...</td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log \\log (1/\\epsilon))$</mathjax><sup id=\"fnref:ee364a-unconstrained\"><a class=\"footnote-ref\" href=\"#fn:ee364a-unconstrained\" rel=\"footnote\">14</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n^3)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Only applicable when <mathjax>$f(x)$</mathjax> is twice differentiable. Constraints can be\n",
    "        incorporated via interior point methods.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Conjugate Gradient Descent</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td>...</td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n^2)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Converges in exactly <mathjax>$n$</mathjax> steps for quadratic <mathjax>$f(x)$</mathjax>. May fail to\n",
    "        converge for non-quadratic objectives.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>L-BFGS</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td>...</td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td>Between <mathjax>$O(\\log (1/\\epsilon))$</mathjax> and <mathjax>$O(\\log \\log (1/\\epsilon))$</mathjax><sup id=\"fnref:ee236c-qnewton\"><a class=\"footnote-ref\" href=\"#fn:ee236c-qnewton\" rel=\"footnote\">15</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n^2)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f(x)$</mathjax> is differentiable, but works best when twice\n",
    "        differentiable. Convergence rate is not guaranteed.\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody></p>\n",
    "</table>\n",
    "<h1>Stochastic Methods</h1>\n",
    "<p>The following algorithms are specifically designed for supervised machine\n",
    "learning where the objective can be decomposed into independent \"loss\"\n",
    "functions and a regularizer,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\min_{x} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(x) + \\lambda g(x)\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>The intuition is that finding the optimal solution to this problem is\n",
    "unnecessary as the goal is to minimize the \"risk\" (read: error) with respect to\n",
    "a set of <em>samples</em> from the true distribution of potential loss functions.\n",
    "Thus, the following algorithms' convergence rates are for the <em>expected</em> rate\n",
    "of convergence (as opposed to the above algorithms which upper bound the <em>true</em>\n",
    "rate of convergence).</p>\n",
    "<table class=\"table table-bordered table-centered\">\n",
    "<p><colgroup>\n",
    "    <col style=\"width:20%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:40%\">\n",
    "  </colgroup></p>\n",
    "<p><thead>\n",
    "    <tr>\n",
    "      <th>Algorithm          </th>\n",
    "      <th>Problem Formulation</th>\n",
    "      <th>Convex             </th>\n",
    "      <th>Strongly Convex    </th>\n",
    "      <th>Per-Iteration Cost </th>\n",
    "      <th>Notes              </th>\n",
    "    </tr>\n",
    "  </thead></p>\n",
    "<p><tbody>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Stochastic Gradient Descent (SGD)</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} \\sum_{i} f_{i}(x) + \\lambda g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(n/\\epsilon^2)$</mathjax><sup id=\"fnref:bach-2012\"><a class=\"footnote-ref\" href=\"#fn:bach-2012\" rel=\"footnote\">16</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(n/\\epsilon)$</mathjax><sup id=\"fnref:bach-2012\"><a class=\"footnote-ref\" href=\"#fn:bach-2012\" rel=\"footnote\">16</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Assumes objective is differentiable.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Stochastic Dual Coordinate Ascent (SDCA)</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} \\sum_{i} f_{i}(x) + \\frac{\\lambda}{2} \\norm{x}_2^2$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(\\frac{1}{\\lambda \\epsilon})$</mathjax><sup id=\"fnref:shalevshwartz-2012\"><a class=\"footnote-ref\" href=\"#fn:shalevshwartz-2012\" rel=\"footnote\">17</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(( \\frac{1}{\\lambda} ) \\log ( \\frac{1}{\\lambda \\epsilon} ))$</mathjax><sup id=\"fnref:shalevshwartz-2012\"><a class=\"footnote-ref\" href=\"#fn:shalevshwartz-2012\" rel=\"footnote\">17</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Accelerated Proximal Stochastic Dual Coordinate Ascent (APSDCA)</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{C}} \\sum_{i} f_{i}(x) + \\lambda g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(\\min (\\frac{1}{\\lambda \\epsilon}, \\sqrt{\\frac{N}{\\lambda \\epsilon}} ))$</mathjax><sup id=\"fnref:shalevshwartz-2013\"><a class=\"footnote-ref\" href=\"#fn:shalevshwartz-2013\" rel=\"footnote\">18</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\min (\\frac{1}{\\lambda}, \\sqrt{\\frac{N}{\\lambda}}) \\log ( \\frac{1}{\\epsilon} ))$</mathjax><sup id=\"fnref:shalevshwartz-2013\"><a class=\"footnote-ref\" href=\"#fn:shalevshwartz-2013\" rel=\"footnote\">18</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Stochastic Average Gradient (SAG)</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} \\sum_{i} f_{i}(x) + \\lambda g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon)$</mathjax><sup id=\"fnref:schmidt-2013\"><a class=\"footnote-ref\" href=\"#fn:schmidt-2013\" rel=\"footnote\">19</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1/\\epsilon))$</mathjax><sup id=\"fnref:schmidt-2013\"><a class=\"footnote-ref\" href=\"#fn:schmidt-2013\" rel=\"footnote\">19</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f_{i}(x)$</mathjax> is differentiable.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Stochastic Variance Reduced Gradient (SVRG)</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} \\sum_{i} f_{i}(x) + \\lambda g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon)$</mathjax><sup id=\"fnref:johnson-2013\"><a class=\"footnote-ref\" href=\"#fn:johnson-2013\" rel=\"footnote\">22</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1/\\epsilon))$</mathjax><sup id=\"fnref:johnson-2013\"><a class=\"footnote-ref\" href=\"#fn:johnson-2013\" rel=\"footnote\">22</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f_{i}(x)$</mathjax> is differentiable.\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>MISO</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathbb{R}^n} \\sum_{i} f_{i}(x) + \\lambda g(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1 / \\epsilon)$</mathjax><sup id=\"fnref:mairal-2013\"><a class=\"footnote-ref\" href=\"#fn:mairal-2013\" rel=\"footnote\">13</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1/\\epsilon))$</mathjax><sup id=\"fnref:mairal-2013\"><a class=\"footnote-ref\" href=\"#fn:mairal-2013\" rel=\"footnote\">13</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$f_{i}(x)$</mathjax> is differentiable. <mathjax>$g(x)$</mathjax> may be used as\n",
    "        a barrier function.\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody></p>\n",
    "</table>\n",
    "<h1>Other Methods</h1>\n",
    "<p>The following methods do not fit well into any of the preceding categories.\n",
    "Included are meta-algorithms like ADMM, which are good for distributing\n",
    "computation across machines, and methods whose per-iteration complexity depends\n",
    "on iteration count <mathjax>$t$</mathjax>.</p>\n",
    "<table class=\"table table-bordered table-centered\">\n",
    "<p><colgroup>\n",
    "    <col style=\"width:20%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:10%\">\n",
    "    <col style=\"width:40%\">\n",
    "  </colgroup></p>\n",
    "<p><thead>\n",
    "    <tr>\n",
    "      <th>Algorithm          </th>\n",
    "      <th>Problem Formulation</th>\n",
    "      <th>Convex             </th>\n",
    "      <th>Strongly Convex    </th>\n",
    "      <th>Per-Iteration Cost </th>\n",
    "      <th>Notes              </th>\n",
    "    </tr>\n",
    "  </thead></p>\n",
    "<p><tbody>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Alternating Direction Method of Multipliers (ADMM)</td>\n",
    "      <!-- Problem            -->\n",
    "      <td>\n",
    "        <mathjax>$$\n",
    "          \\begin{align*}\n",
    "            \\min_{x,z} \\quad\n",
    "              & f(x) + g(z) \\\\\n",
    "            \\text{s.t.} \\quad\n",
    "              & Ax + Bz = c\n",
    "          \\end{align*}\n",
    "        $$</mathjax>\n",
    "      </td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1/\\epsilon)$</mathjax><sup id=\"fnref:blog-admm\"><a class=\"footnote-ref\" href=\"#fn:blog-admm\" rel=\"footnote\">7</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1/\\epsilon))$</mathjax><sup id=\"fnref:hong-2012\"><a class=\"footnote-ref\" href=\"#fn:hong-2012\" rel=\"footnote\">21</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(n)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        The stated convergence rate for \"Strongly Convex\" only requires <mathjax>$f(x)$</mathjax> to\n",
    "        be strongly convex, not <mathjax>$g(x)$</mathjax>. This same rate can also be applied to\n",
    "        the \"Convex\" case under several non-standard assumptions<sup id=\"fnref:hong-2012\"><a class=\"footnote-ref\" href=\"#fn:hong-2012\" rel=\"footnote\">21</a></sup>.\n",
    "        Matrices <mathjax>$A$</mathjax> and <mathjax>$B$</mathjax> may also need to be full column rank<sup id=\"fnref:deng-2012\"><a class=\"footnote-ref\" href=\"#fn:deng-2012\" rel=\"footnote\">20</a></sup> .\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Bundle Method</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathcal{C}} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(1/\\epsilon)$</mathjax><sup id=\"fnref:smola-2007\"><a class=\"footnote-ref\" href=\"#fn:smola-2007\" rel=\"footnote\">23</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:smola-2007\"><a class=\"footnote-ref\" href=\"#fn:smola-2007\" rel=\"footnote\">23</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td><mathjax>$O(tn)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <!-- Algorithm          -->\n",
    "      <td>Center of Gravity Algorithm</td>\n",
    "      <!-- Problem            -->\n",
    "      <td><mathjax>$\\displaystyle \\min_{x \\in \\mathcal{C}} f(x)$</mathjax></td>\n",
    "      <!-- Convex             -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:ee236c-localization\"><a class=\"footnote-ref\" href=\"#fn:ee236c-localization\" rel=\"footnote\">24</a></sup></td>\n",
    "      <!-- Strongly Convex    -->\n",
    "      <td><mathjax>$O(\\log (1 / \\epsilon))$</mathjax><sup id=\"fnref:ee236c-localization\"><a class=\"footnote-ref\" href=\"#fn:ee236c-localization\" rel=\"footnote\">24</a></sup></td>\n",
    "      <!-- Per-Iteration Cost -->\n",
    "      <td>At least <mathjax>$O(tn)$</mathjax></td>\n",
    "      <!-- Notes              -->\n",
    "      <td>\n",
    "        Applicable when <mathjax>$\\mathcal{C}$</mathjax> is bounded. Each iteration requires\n",
    "        finding a near-central point in a convex set; this may be\n",
    "        computationally expensive.\n",
    "      </td>\n",
    "    </tr></p>\n",
    "</tbody>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
