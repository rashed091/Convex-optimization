{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.\n",
    "At the same time, he established the Accelerated Gradient Method, proved that\n",
    "its convergence rate superior to Gradient Descent (<mathjax>$O(1/\\sqrt{\\epsilon})$</mathjax>\n",
    "iterations instead of <mathjax>$O(1/\\epsilon)$</mathjax>), and then proved that no other\n",
    "first-order (that is, gradient-based) algorithm could ever hope to beat it.\n",
    "What a boss.</p>\n",
    "<p><a href=\"http://stronglyconvex.com/blog/gradient-descent.html\">Continuing</a> <a href=\"http://stronglyconvex.com/blog/subgradient-descent.html\">my analogy</a>, imagine\n",
    "that you are standing on the side of a mountain and want to reach the bottom.\n",
    "If you were to follow the Accelerated Gradient Method, you'd do something like\n",
    "this,</p>\n",
    "<div class=\"pseudocode\">\n",
    "<ol>\n",
    "<li>Look around you and see which way points the most dowards</li>\n",
    "<li>Take a step in that direction</li>\n",
    "<li>Take another step in that direction, even if it starts taking you uphill.\n",
    "     Then repeat.</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<p>As we'll see, that last unintuitive bit is the key to Accelerated Gradient\n",
    "Descent's speed.</p>\n",
    "<h1><a name=\"implementation\" href=\"#implementation\">How does it work?</a></h1>\n",
    "<p>We begin by assuming the we want to minimize an unconstrained function <mathjax>$f$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  \\min_{x} \\, f(x)\n",
    "$$</mathjax></p>\n",
    "<p>As with Gradient Descent, we'll assume that <mathjax>$f$</mathjax> is differentiable and that we\n",
    "can easily compute its gradient <mathjax>$\\nabla f(x)$</mathjax>. Then the Accelerated Gradient\n",
    "Method is defined as,</p>\n",
    "<div class=\"pseudocode\">\n",
    "<p><strong>Input</strong>: initial iterate <mathjax>$y^{(0)}$</mathjax></p>\n",
    "<ol>\n",
    "<li>For <mathjax>$t = 1, 2, \\ldots$</mathjax><ol>\n",
    "<li>Let <mathjax>$x^{(t)} = y^{(t-1)} - \\alpha^{(t)} \\nabla f(y^{(t-1)})$</mathjax></li>\n",
    "<li>if converged, return <mathjax>$x^{(t)}$</mathjax></li>\n",
    "<li>Let <mathjax>$y^{(t)} = x^{(t)} + \\frac{t-1}{t+2} (x^{(t)} - x^{(t-1)})$</mathjax></li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</div>\n",
    "<h1><a name=\"example\" href=\"#example\">A Small Example</a></h1>\n",
    "<p>We'll try out Accelerated Gradient on the <a href=\"/blog/gradient-descent.html#example\">same example used to showcase\n",
    "Gradient Descent</a>. We'll use the objective function\n",
    "<mathjax>$f(x) = x^4$</mathjax>, meaning that <mathjax>$\\nabla_x f(x) = 4 x^3$</mathjax>. For a step size, we'll use\n",
    "Backtracking Line Search where the largest acceptable step size is <mathjax>$0.05$</mathjax>.\n",
    "Finally, we'll start at <mathjax>$x^{(0)} = 1$</mathjax>.  Compare these 2 graphs to the ones for\n",
    "Gradient Descent.</p>\n",
    "\n",
    "<div class=\"img-center\">\n",
    "  <img src=\"../images/convergence3.png\"/>\n",
    "  <span class=\"caption\">\n",
    "    This plot shows how quickly the objective function decreases as the\n",
    "    number of iterations increases.\n",
    "  </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"img-center\">\n",
    "  <img src=\"../images/iterates3.png\"/>\n",
    "  <span class=\"caption\">\n",
    "    This plot shows the actual iterates and the objective function evaluated at\n",
    "    those points. More red indicates a higher iteration number.\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><a name=\"proof\" href=\"#proof\">Why does it work?</a></h1>\n",
    "<p>The assumptions for the Accelerated Gradient Method are identical to Gradient\n",
    "Descent's. In particular, we assume,</p>\n",
    "<ol>\n",
    "<li><mathjax>$f$</mathjax> is convex, differentiable, and finite for all <mathjax>$x$</mathjax></li>\n",
    "<li>a finite solution <mathjax>$x^{*}$</mathjax> exists</li>\n",
    "<li><mathjax>$\\nabla f(x)$</mathjax> is Lipschitz continuous with constant <mathjax>$L$</mathjax>. That is, there must\n",
    "   be an <mathjax>$L$</mathjax> such that,</li>\n",
    "</ol>\n",
    "<p><mathjax>$$\n",
    "  || \\nabla f(x) - \\nabla f(y) ||_2 \\le L || x - y ||_2 \\qquad \\forall x,y\n",
    "$$</mathjax></p>\n",
    "<p><strong>Assumptions</strong> As explained in my post on <a href=\"http://stronglyconvex.com/blog/gradient-descent.html\">Gradient\n",
    "Descent</a>, these assumptions give us 2 things. Assumption 1\n",
    "gives us a linear lower bound on <mathjax>$f$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  f(y) \\ge f(x) + \\nabla f(x)^T (y-x) \\qquad \\forall x, y\n",
    "$$</mathjax></p>\n",
    "<p>Assumption 3 then gives us a quadratic upper bound on <mathjax>$f$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  f(y) \\le f(x) + \\nabla f(x)^T (y-x) + \\frac{L}{2} ||x - y||_2^2 \\qquad \\forall x, y\n",
    "$$</mathjax></p>\n",
    "<p>Both of these will prove critical in the following proof.</p>\n",
    "<p><strong>Proof Outline</strong> The proof for the Accelerated Gradient Method is the\n",
    "trickiest one yet. We'll see that everything fits into place just right, but\n",
    "that there's very little intuition as to where it all came from.</p>\n",
    "<p>We begin in Step 1 by defining a third iterate <mathjax>$v^{(t)}$</mathjax> that's a\n",
    "linear combination of <mathjax>$x^{(t)}$</mathjax> and <mathjax>$x^{(t-1)}$</mathjax>. These iterates are purely for\n",
    "the sake of the proof and are not computed during the algorithm. Using these\n",
    "iterates, we upper bound <mathjax>$f(x^{(t+1)})$</mathjax> in terms of <mathjax>$f(x^{(t)})$</mathjax>, <mathjax>$f(x^{*})$</mathjax>,\n",
    "the distance between <mathjax>$v^{(t+1)}$</mathjax> and <mathjax>$x^{*}$</mathjax>, and the distance between\n",
    "<mathjax>$v^{(t)}$</mathjax> and <mathjax>$x^{*}$</mathjax>.</p>\n",
    "<p>Step 2 involves upper bounding the error <mathjax>$f(x^{(t+1)}) - f(x^{*})$</mathjax> by <mathjax>$f(x^{0})\n",
    "- f(x^{*})$</mathjax> and the distance between <mathjax>$v^{(0)}$</mathjax> and <mathjax>$x^{*}$</mathjax> using our very\n",
    "careful choice of <mathjax>$\\frac{t-1}{t+2}$</mathjax>.</p>\n",
    "<p>Finally, Step 3 brings it all together by bounding <mathjax>$f(x^{(t+1)}) - f(x^{*})$</mathjax> by\n",
    "a term depending on <mathjax>$1/(t+2)^2$</mathjax>.</p>\n",
    "<p><strong>Step 1</strong> upper bounding <mathjax>$f(x^{(t+1)})$</mathjax>.  First, define the following and assume that <mathjax>$\\theta^{(0)} = 1$</mathjax>, <mathjax>$v^{(0)} = x^{(0)}$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  v^{(t)}\n",
    "  = \\frac{t+1}{2} x^{(t)} - \\frac{t-1}{2} x^{(t-1)} \\qquad\n",
    "  \\theta^{(t)}\n",
    "  = \\frac{2}{t+2}\n",
    "$$</mathjax></p>\n",
    "<p>There are 2 consequences of this definition for <mathjax>$v^{(t)}$</mathjax>. The first is that <mathjax>$y^{(t)}$</mathjax> is a linear combination of <mathjax>$v^{(t)}$</mathjax> and <mathjax>$x^{(t)}$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\color{fuchsia} y^{(t)}\n",
    "  & = x^{(t)} + \\frac{t-1}{t+2} ( x^{(t)} - x^{(t-1)} ) \\\\\n",
    "  & = \\frac{(t+2) + (t-1)}{t+2} x^{(t)} - \\frac{t-1}{t+2} x^{(t-1)} \\\\\n",
    "  & = \\frac{t+1}{t+2} x^{(t)} - \\frac{t-1}{t+2} x^{(t-1)} + \\frac{t}{t+2} x^{(t)} \\\\\n",
    "  & = \\frac{2}{t+2} \\left( \\frac{t+1}{2} x^{(t)} - \\frac{t-1}{2} x^{(t-1)} \\right) + (1 - \\frac{2}{t+2}) x^{(t)} \\\\\n",
    "  & = \\color{fuchsia} \\theta^{(t)} v^{(t)} + (1 - \\theta^{(t)}) x^{(t)} \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>The second is that <mathjax>$v^{(t)}$</mathjax> is a gradient step on <mathjax>$v^{(t)}$</mathjax>, except that the gradient is evaluated at <mathjax>$y^{(t)}$</mathjax> (work backwards from the following),</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\color{OrangeRed} v^{(t+1)}\n",
    "  & = \\color{OrangeRed} v^{(t)} - \\frac{\\alpha^{(t+1)}}{\\theta^{(t)}} \\nabla f(y^{(t)}) \\\\\n",
    "  & = v^{(t)} + \\frac{1}{\\theta^{(t)}} ( y^{(t)} - \\alpha^{(t+1)} \\nabla f(y^{(t)}) - y^{(t)} ) \\\\\n",
    "  & = v^{(t)} + \\frac{1}{\\theta^{(t)}} ( x^{(t+1)} - y^{(t)} ) \\\\\n",
    "  \\frac{t+2}{2} x^{(t+1)} - \\frac{t}{2} x^{(t)}\n",
    "  & = \\frac{t+1}{2} x^{(t)} - \\frac{t-1}{2} x^{(t-1)} + \\frac{1}{\\theta^{(t)}} ( x^{(t+1)} - y^{(t)} ) \\\\\n",
    "  & = \\frac{t+1}{2} x^{(t)} - \\frac{t-1}{2} x^{(t-1)} + \\frac{t+2}{2} ( x^{(t+1)} - y^{(t)} ) \\\\\n",
    "  - \\frac{t}{2} x^{(t)}\n",
    "  & = \\frac{t+1}{2} x^{(t)} - \\frac{t-1}{2} x^{(t-1)} - \\frac{t+2}{2} y^{(t)} \\\\\n",
    "  y^{(t)}\n",
    "  & = \\frac{2t+1}{t+2} x^{(t)} - \\frac{t-1}{t+2} x^{(t-1)} \\\\\n",
    "  & = x^{(t)} + \\frac{t-1}{t+2} ( x^{(t)} - x^{(t-1)} )\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>With these 2 properties in hand, let's upper bound <mathjax>$f(x^{(t)})$</mathjax>.  Let <mathjax>$\\alpha^{(t)} \\le \\frac{1}{L}$</mathjax> and define\n",
    "<mathjax>$x^{+} \\triangleq x^{(t)}$</mathjax>,\n",
    "<mathjax>$x \\triangleq x^{(t-1)}$</mathjax>,\n",
    "<mathjax>$v^{+} \\triangleq v^{(t)}$</mathjax>,\n",
    "<mathjax>$v \\triangleq v^{(t-1)}$</mathjax>,\n",
    "<mathjax>$\\alpha^{+} \\triangleq \\alpha^{(t)}$</mathjax>,\n",
    "<mathjax>$y \\triangleq y^{(t-1)}$</mathjax>,\n",
    "<mathjax>$\\theta \\triangleq \\theta^{(t-1)}$</mathjax></p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{+})\n",
    "  & = f(y - \\alpha^{+} \\nabla f(y)) \\\\\n",
    "  & \\le {\\color{red} f(y) - \\frac{\\alpha^{+}}{2} || \\nabla f(y) ||_2^2} \\\\\n",
    "  & \\le {\\color{blue} (1-\\theta) f(x) + \\theta f(x^{*}) + \\nabla f(y)^T (y - (1-\\theta) x + \\theta x^{*}) } - \\frac{\\alpha^{+}}{2} || \\nabla f(y) ||_2^2 \\\\\n",
    "  & = (1-\\theta) f(x) + \\theta f(x^{*}) + \\nabla f(y)^T ({\\color{fuchsia} \\theta v} + \\theta x^{*}) - \\frac{\\alpha^{+}}{2} || \\nabla f(y) ||_2^2 \\\\\n",
    "  & = (1-\\theta) f(x) + \\theta f(x^{*}) + \\frac{\\theta^2}{2 \\alpha^{+}} \\left(\n",
    "        \\frac{2 \\alpha^{+}}{\\theta} \\nabla f(y)^T (v-x) - \\left( \\frac{\\alpha^{+}}{\\theta} \\right)^2 || \\nabla f(y) ||_2^2\n",
    "        \\pm ||v - x^{*}||_2^2\n",
    "      \\right) \\\\\n",
    "  & = (1-\\theta) f(x) + \\theta f(x^{*}) + \\frac{\\theta^2}{2 \\alpha^{+}} \\left(\n",
    "        ||v - x^{*}||_2^2 - ||v - x^{*} - \\frac{\\alpha^{+}}{\\theta} \\nabla f(y)||_2^2 \n",
    "      \\right) \\\\\n",
    "  & = (1-\\theta) f(x) + \\theta f(x^{*}) + \\frac{\\theta^2}{2 \\alpha^{+}} \\left(\n",
    "        ||v - x^{*}||_2^2 - ||{\\color{OrangeRed} v^{+} } - x^{*}||_2^2\n",
    "      \\right) \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Using the Lipschitz assumption on <mathjax>$||\\nabla f(y)||_2$</mathjax> and that <mathjax>$\\alpha^{+} \\le \\frac{1}{L}$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\color{red}\n",
    "  f(y - \\alpha^{+} \\nabla f(y))\n",
    "  & \\le f(y) + \\nabla f(y)^T (y - \\alpha^{+} \\nabla f(y) - y) + \\frac{L}{2}{ ||y - \\alpha^{+} \\nabla f(y) - y ||_2^2 } \\\\\n",
    "  & = f(y) - \\alpha^{+} || \\nabla f(y) ||_2^2 + \\frac{L (\\alpha^{+})^2}{2} || \\nabla f(y) ||_2^2 \\\\\n",
    "  & = f(y) - \\frac{\\alpha^{+}}{2} ( 2 - L \\alpha^{+} ) || \\nabla f(y) ||_2^2 \\\\\n",
    "  & \\le \\color{red} f(y) - \\frac{\\alpha^{+}}{2} || \\nabla f(y) ||_2^2 \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Using convexity's linear lower bound and its line-over-function properties,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\color{blue}\n",
    "  f( (1-\\theta)x + \\theta x^{*} )\n",
    "  & \\ge f(y) + \\nabla f(y)^T ( (1-\\theta)x + \\theta x^{*} - y) \\\\\n",
    "  (1-\\theta) f(x) + \\theta f(x^{*}) \n",
    "  & \\ge f(y) + \\nabla f(y)^T ( (1-\\theta)x + \\theta x^{*} - y) \\\\\n",
    "  f(y)\n",
    "  & \\le \\color{blue} (1-\\theta) f(x) + \\theta f(x^{*}) + \\nabla f(y)^T(y - (1-\\theta) x + \\theta x^{*} )\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p><strong>Step 2</strong> Upper bounding <mathjax>$f(x^{(t+1)}) - f(x^{*})$</mathjax>. We begin by manipulating the result from step 1 into something such that the left and right side look almost the same,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{(t+1)})\n",
    "  & \\le (1-\\theta^{(t)}) f(x^{(t)}) + \\theta^{(t)} f(x^{*}) \\\\\n",
    "  & \\quad + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} \\left(\n",
    "        ||v^{(t)} - x^{*}||_2^2 - \\underbrace{ ||v^{(t+1)} - x^{*}||_2^2 }_{\\text{move to other side}}\n",
    "     \\right) \\\\\n",
    "  f(x^{(t+1)}) + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} ||v^{(t+1)} - x^{*}||_2^2\n",
    "  & \\le (1-\\theta^{(t)}) f(x^{(t)}) + \\theta^{(t)} f(x^{*}) \\\\\n",
    "  & \\quad + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} ||v^{(t)} - x^{*}||_2^2 \\underbrace{ \\pm f(x^{*}) }_{=0} \\\\\n",
    "  f(x^{(t+1)}) - f(x^{*}) + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} ||v^{(t+1)} - x^{*}||_2^2\n",
    "  & \\le (1-\\theta^{(t)}) f(x^{(t)}) \\underbrace{ - f(x^{*}) + \\theta^{(t)} f(x^{*}) }_{\\text{group together}} \\\\\n",
    "  & \\quad + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} ||v^{(t)} - x^{*}||_2^2 \\\\\n",
    "  f(x^{(t+1)}) - f(x^{*}) + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} ||v^{(t+1)} - x^{*}||_2^2\n",
    "  & \\le (1-\\theta^{(t)}) f(x^{(t)}) - (1-\\theta^{(t)}) f(x^{*}) \\\\\n",
    "  & \\quad + \\frac{ (\\theta^{(t)})^2}{2 \\alpha^{(t+1)}} ||v^{(t)} - x^{*}||_2^2 \\\\\n",
    "  \\frac{1}{( \\theta^{(t)} )^2} \\left( f(x^{(t+1)}) - f(x^{*}) \\right) + \\frac{1}{2 \\alpha^{(t+1)}} ||v^{(t+1)} - x^{*}||_2^2\n",
    "  & \\le \\frac{ (1-\\theta^{(t)}) }{ (\\theta^{(t)})^2 } \\left( f(x^{(t)}) - f(x^{*}) \\right) \\\\\n",
    "  & \\quad + \\frac{1}{2 \\alpha^{(t+1)}} ||v^{(t)} - x^{*}||_2^2 \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Remember how <mathjax>$\\theta^{(t)} = \\frac{2}{t+2}$</mathjax>? Well that means <mathjax>$\\theta^{(t)} \\gt 0$</mathjax> and thus <mathjax>$\\frac{1 - \\theta^{(t)}}{ (\\theta^{(t)})^2 } \\le \\frac{1}{ (\\theta^{(t)})^2 }$</mathjax>. Subbing that into the previous equation lets us apply it recursively to obtain,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\frac{1}{( \\theta^{(t)} )^2} \\left( f(x^{(t+1)}) - f(x^{*}) \\right) + \\frac{1}{2 \\alpha^{(t+1)}} ||v^{(t+1)} - x^{*}||_2^2\n",
    "  & \\le \\frac{ (1-\\theta^{(0)}) }{ (\\theta^{(0)})^2 } \\left( f(x^{(0)}) - f(x^{*}) \\right) + \\frac{1}{2 \\alpha^{(1)}} ||v^{(0)} - x^{*}||_2^2 \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p><strong>Step 3</strong> Bound the error in terms of <mathjax>$\\frac{1}{(t+2)^2}$</mathjax>.  Recall that <mathjax>$\\theta^{(0)} = 1$</mathjax> and <mathjax>$v^{(0)} = x^{(0)}$</mathjax>. Then,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\frac{1}{( \\theta^{(t)} )^2} \\left( f(x^{(t+1)}) - f(x^{*}) \\right) \n",
    "    + \\underbrace{ \\frac{1}{2 \\alpha^{(t+1)}} ||v^{(t+1)} - x^{*}||_2^2 }_{\\ge 0 \\text{, so drop}}\n",
    "  & \\le \\underbrace{ \\frac{ (1-\\theta^{(0)}) }{ (\\theta^{(0)})^2 } }_{= 0} \\left( f(x^{(0)}) - f(x^{*}) \\right)\n",
    "    + \\frac{1}{2 \\alpha^{(1)}} ||\\underbrace{ v^{(0)} }_{x^{(0)}} - x^{*}||_2^2 \\\\\n",
    "  \\frac{1}{( \\theta^{(t)} )^2} \\left( f(x^{(t+1)}) - f(x^{*}) \\right)\n",
    "  & \\le \\frac{1}{2 \\alpha^{(1)}} || x^{(0)} - x^{*} ||_2^2 \\\\\n",
    "  f(x^{(t+1)}) - f(x^{*})\n",
    "  & \\le \\frac{( \\theta^{(t)} )^2}{2 \\alpha^{(1)}} || x^{(0)} - x^{*} ||_2^2 \\\\\n",
    "  & = \\frac{2}{ (t+2)^2 \\alpha^{(1)} } || x^{(0)} - x^{*} ||_2^2\n",
    "\\end{align*}\n",
    "$$</mathjax></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Thus, the convergence rate of the Accelerated Gradient Method is\n",
    "<mathjax>$O(\\frac{1}{\\sqrt{\\epsilon}})$</mathjax>. Woo!</p>\n",
    "<p><a id=\"usage\"></a></p>\n",
    "<h1><a name=\"usage\" href=\"#usage\">When should I use it?</a></h1>\n",
    "<p>The Accelerated Gradient Method trumps Gradient Descent in every way theoretically, but the latter is still more widely used and preferred.  Why? The fact is that Accelerated Gradient is much more of a pain to implement. Whereas with Gradient Descent you can simply check if your new iterate's score is less than the previous one, Accelerated Gradient's score may increase before decreasing again. Accelerated Gradient is also extremely sensitive to step size -- if <mathjax>$\\alpha^{(t)}$</mathjax> isn't in <mathjax>$(0, \\frac{1}{L}]$</mathjax>, <em>it will diverge</em>. Accelerated Gradient is a powerful but fickle tool. Use it when you can, but keep Gradient Descent handy if everything is going awry.</p>\n",
    "<h1><a name=\"extensions\" href=\"#extensions\">Extensions</a></h1>\n",
    "<p><strong>Step Size</strong> As mentioned previously, Accelerated Gradient is very fickle\n",
    "with step size.  While Backtracking Line Search can and should be used when\n",
    "possible, it is essential that the constants be set such that <mathjax>$\\alpha^{(t+1)}\n",
    "\\le \\frac{1}{L}$</mathjax>. In other words, when,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(y^{(t)} - \\alpha^{(t+1)} \\nabla f(y^{(t)}))\n",
    "  & \\le f(y^{(t)}) + \\nabla f(y^{(t)})^T ((y^{(t)} - \\alpha^{(t+1)} \\nabla f(y^{(t)})) - y^{(t)}) \\\\\n",
    "  & \\quad + \\frac{1}{2 \\alpha^{(t+1)} } ||(y^{(t)} - \\alpha^{(t+1)} \\nabla f(y^{(t)})) - y^{(t)}||_2^2 \\\\\n",
    "  & = f(y^{(t)}) - \\alpha^{(t+1)} || \\nabla f(y^{(t)}) ||_2^2 + \\frac{ \\alpha^{(t+1)} }{2} ||\\nabla f(y^{(t)}) ||_2^2 \\\\\n",
    "  & = f(y^{(t)}) - \\frac{\\alpha^{(t+1)}}{2} || \\nabla f(y^{(t)}) ||_2^2 \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Typically the <mathjax>$\\frac{1}{2}$</mathjax> in the last part of the last line is traded for another constant. <em>This will not work for Accelerated Gradient!</em></p>\n",
    "<p><strong>Checking Convergence</strong> As with Gradient Descent and Subgradient Descent,\n",
    "there is no real way to be certain when <mathjax>$f(x^{(t)}) - f(x^{*}) \\lt \\epsilon$</mathjax>\n",
    "without some problem-specific knowledge. Instead, it is common to stop after a\n",
    "fixed number of iterations or when <mathjax>$||\\nabla f(x^{(t)})||_2$</mathjax> is small.</p>\n",
    "<h1><a name=\"references\" href=\"#references\">References</a></h1>\n",
    "<p><strong>Proof of Convergence</strong> The proof of convergence is thanks to Lieven\n",
    "Vandenberghe's <a href=\"http://math.sjtu.edu.cn/faculty/zw2109/course/sp04-2-gradient.pdf\">EE236c slides</a> hosted by Zaiwen Wen.</p>\n",
    "<h1><a name=\"reference-impl\" href=\"#reference-impl\">Reference Implementation</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07d058eac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def accelerated_gradient(gradient, y0, alpha, n_iterations=100):\n",
    "    ys = [y0]\n",
    "    xs = [y0]\n",
    "    for t in range(1, n_iterations + 1):\n",
    "        y = ys[-1]\n",
    "        x = xs[-1]\n",
    "        g = gradient(y)\n",
    "        x_plus = y - alpha(y, g) * g\n",
    "        y_plus = x_plus + ((t - 1) / float(t + 2)) * (x_plus - x)\n",
    "        ys.append(y_plus)\n",
    "        xs.append(x_plus)\n",
    "    return xs\n",
    "\n",
    "\n",
    "class BacktrackingLineSearch(object):\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "        self.alpha = 0.05\n",
    "\n",
    "    def __call__(self, y, g):\n",
    "        f = self.function\n",
    "        a = self.alpha\n",
    "        while f(y - a * g) > f(y) - 0.5 * a * (g * g):\n",
    "            a *= 0.99\n",
    "        return a\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ### ACCELERATED GRADIENT ###\n",
    "\n",
    "    # problem definition\n",
    "    function = lambda x: x**4  # the function to minimize\n",
    "    gradient = lambda x: 4 * x**3  # its gradient\n",
    "    alpha = BacktrackingLineSearch(function)\n",
    "    x0 = 1.0\n",
    "    n_iterations = 10\n",
    "\n",
    "    # run gradient descent\n",
    "    iterates = accelerated_gradient(gradient, x0, alpha, n_iterations=n_iterations)\n",
    "\n",
    "    ### PLOTTING ###\n",
    "    plt.plot(iterates)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
