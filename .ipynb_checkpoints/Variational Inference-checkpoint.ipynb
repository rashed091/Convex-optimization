{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently the two chief ways of doing approximate Bayesian inference. In the\n",
    "Bayesian setting, we typically have some observed variables <mathjax>$x$</mathjax> and\n",
    "unobserved variables <mathjax>$z$</mathjax>, and our goal is to calculate <mathjax>$P(z|x)$</mathjax>. In all but\n",
    "the simplest cases, calculating <mathjax>$P(z|x)$</mathjax> for all values of <mathjax>$z$</mathjax> in closed form\n",
    "is impossible, so approximations must be made.</p>\n",
    "<p>Variational Inference's approximation is made by choosing a family of\n",
    "distributions <mathjax>$q(z|\\eta)$</mathjax> parameterized by <mathjax>$\\eta$</mathjax> and choosing a setting for\n",
    "<mathjax>$\\eta$</mathjax> that brings <mathjax>$q(z|\\eta)$</mathjax> \"close\" to <mathjax>$P(z|x)$</mathjax>.  In particular,\n",
    "Variational Inference is about finding,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\arg\\min_{\\eta} KL \\left[ q(z|\\eta) || P(z|x) \\right] \\\\\n",
    "  &amp; = \\arg\\min_{\\eta} \\sum_{z} q(z|\\eta) \\log \\frac{ q(z|\\eta) }{ P(z|x) }\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Looking at this formulation, the first thing you should be thinking is, \"We\n",
    "don't even know how to calculate <mathjax>$P(z|x)$</mathjax> much less take an expectation with\n",
    "respect to it. How can I possibly solve this problem?\" The key is to restrict\n",
    "<mathjax>$q(z|\\eta)$</mathjax> to decompose into a product of independent distributions, 1 for\n",
    "each hidden variable <mathjax>$z_i$</mathjax>. In other words,</p>\n",
    "<p><mathjax>$$\n",
    "  q(z|\\eta) = \\prod_{i} q(z_i | \\eta_i)\n",
    "$$</mathjax></p>\n",
    "<p>This is the \"mean field approximation\" and will allow us to optimize each\n",
    "<mathjax>$\\eta_i$</mathjax> one at a time. The final key <mathjax>$P(z_i|z_{-i},x)$</mathjax> must lie in the\n",
    "exponential family, and that <mathjax>$q(z_i|\\eta_i)$</mathjax> be of the same form. For example,\n",
    "if the former is a Dirichlet distribution, so should the latter. When this is\n",
    "the case, we can solve the Coordinate Ascent update in closed form.</p>\n",
    "<p>When all 3 conditions are met -- the mean field approximation, the univariate\n",
    "posteriors lie in the exponential family, and that the individual variational\n",
    "distributions match -- we can apply Coordinate Ascent to minimize the\n",
    "KL-divergence between the mean field distribution and the posterior.</p>\n",
    "<h1><a name=\"derivation\" href=\"#derivation\">Derivation of the Objective</a></h1>\n",
    "<p>The original intuition for Variational Inference stems from lower bounding\n",
    "the marginal likelihood of the observed variables <mathjax>$P(x)$</mathjax>, then maximizing that\n",
    "lower bound. For many choices of <mathjax>$q(z|\\eta)$</mathjax> doing this will be computationally\n",
    "infeasible, but we'll see that if we make the mean field approximation and\n",
    "choose the right variational distributions, then we can efficiently do\n",
    "Coordinate Ascent.</p>\n",
    "<p>First, let's derive a lower bound on the likelihood of the observed\n",
    "variables,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\log P(x)\n",
    "  &amp; = \\log \\left(\n",
    "    \\sum_{z} P(x, z) \\frac{ q(z | \\eta) } { q(z | \\eta) }\n",
    "  \\right) \\\\\n",
    "  &amp; = \\log \\left(\n",
    "    P(x)  \\sum_{z} q(z | \\eta) \\frac{ P(z | x) } { q(z | \\eta) }\n",
    "  \\right) \\\\\n",
    "  &amp; = \\log \\left(\n",
    "    \\sum_{z} q(z | \\eta) \\frac{ P(z | x) } { q(z | \\eta) }\n",
    "  \\right) + \\log P(x) \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Since <mathjax>$\\log$</mathjax> is a concave function, we can apply Jensen's inequality to see\n",
    "that <mathjax>$\\log(p x + (1-p)y) \\ge p \\log(x) + (1-p) \\log y$</mathjax> for any <mathjax>$p \\in [0,\n",
    "1]$</mathjax>.</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\log P(x)\n",
    "  &amp; = \\log \\left(\n",
    "    \\sum_{z} q(z | \\eta) \\frac{ P(z | x) } { q(z | \\eta) }\n",
    "  \\right) + \\log P(x) \\\\\n",
    "  &amp; \\ge \\sum_{z} q(z | \\eta) \\log \\left(\n",
    "    \\frac{ P(z | x) } { q(z | \\eta) }\n",
    "  \\right) + \\log P(x) \\\\\n",
    "  &amp; = - \\sum_{z} q(z | \\eta) \\log \\left(\n",
    "    \\frac{ q(z | \\eta) } { P(z | x) }\n",
    "  \\right) + \\log P(x) \\\\\n",
    "  &amp; = - \\text{KL}[ q(z | \\eta) || P(z | x) ] + \\log P(x) \\\\\n",
    "  &amp; = - \\text{KL}[ q(z | \\eta) || P(z , x) ] \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>From this expression, we can see that minimizing the KL divergence over\n",
    "<mathjax>$\\eta$</mathjax>, we're lower bounding the likelihood of the observed variables.\n",
    "In addition, if <mathjax>$q(z|\\eta)$</mathjax> has the same form as <mathjax>$P(z|x)$</mathjax>, then the best choice\n",
    "for <mathjax>$\\eta$</mathjax> is one that lets <mathjax>$q(z|\\eta) = P(z|x)$</mathjax> for all <mathjax>$z$</mathjax>.</p>\n",
    "<p>At this point, we still have an intractable problem. Even evaluating the KL\n",
    "divergence requires taking an expectation over all settings for <mathjax>$z$</mathjax> (an\n",
    "exponential number in <mathjax>$z$</mathjax>'s length!), so applying an iterative algorithm to\n",
    "choose <mathjax>$\\eta$</mathjax> is right out. However, we'll soon see that by restricting the\n",
    "form of <mathjax>$q(z|\\eta)$</mathjax>, we can potentially decompose the KL divergence into more\n",
    "easily manageable bits.</p>\n",
    "<h1><a name=\"mean-field\" href=\"#mean-field\">The Mean Field Approximation</a></h1>\n",
    "<p>The key to avoiding the massive sum of the previous equation is to assume that\n",
    "<mathjax>$q(z|\\eta)$</mathjax> decomposes into a product of independent distributions. This is\n",
    "known as the \"Mean Field Approximation\". Mathematically, the approximation\n",
    "means that,</p>\n",
    "<p><mathjax>$$\n",
    "  q(z|\\eta) = \\prod_{i} q(z_i | \\eta_i)\n",
    "$$</mathjax></p>\n",
    "<p>Suppose we make this assumption and that we want to perform coordinate ascent\n",
    "on a single index <mathjax>$\\eta_k$</mathjax>. By factoring <mathjax>$P(z|x) = \\prod_{i=1}^{k} P(z_i |\n",
    "z_{1:i-1}, x)$</mathjax> and dropping all terms that are constant with respect to\n",
    "<mathjax>$\\eta_k$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\arg\\max_{\\eta_k} -KL \\left[ q(z|\\eta) || p(z|x) \\right] + \\underbrace{\\log P(x)}_{\\text{constant wrt $\\eta_k$}} \\\\\n",
    "  &amp; = \\arg\\max_{\\eta_k} \\sum_{z} q(z|\\eta) \\log P(z|x) - \\sum_{z} q(z|\\eta) \\log q(z|\\eta) \\\\\n",
    "  &amp; = \\arg\\max_{\\eta_k} \\sum_{z} q(z|\\eta) \\log \\left( \\prod_{i} P(z_{i}|z_{1:i-1},x) \\right)\n",
    "    - \\sum_{z} \\left( \\prod_{i} q(z_i|\\eta_i) \\right) \\log \\left( \\prod_{i} q(z_i|\\eta_i) \\right) \\\\\n",
    "  &amp; = \\arg\\max_{\\eta_k} \\sum_{j} \\sum_{z} q(z|\\eta)\\log P(z_{j}|z_{1:j-1},x)\n",
    "    - \\underbrace{ \\sum_{j} \\sum_{z_j} q(z_j|\\eta_j) \\log q(z_j|\\eta_j) }_{\\text{only $j=k$ not const wrt. $\\eta_k$}} \\\\\n",
    "  &amp; = \\arg\\max_{\\eta_k} \\underbrace{ \\sum_{j} \\sum_{z_{1:j}} \\left( \\prod_{i \\le j} q(z_i|\\eta_i) \\right) \\log P(z_{j}|z_{1:j-1},x) }_{\\text{only last $j$ contains $q(z_k|\\eta_k)$}}\n",
    "    - \\sum_{z_k} q(z_k|\\eta_k) \\log q(z_k|\\eta_k)  \\\\\n",
    "  &amp; = \\arg\\max_{\\eta_k} \\sum_{z} q(z_k|\\eta_k) \\underbrace{ \\left( \\prod_{i \\ne k} q(z_i|\\eta_i) \\right) }_{\\text{fixed wrt $\\eta_k$}} \\log P(z_k | z_{-k}, x)\n",
    "    - \\sum_{z_k} q(z_k|\\eta_k) \\log q(z_k|\\eta_k)  \\\\\n",
    "  &amp; = \\arg\\max_{\\eta_k} \\mathbb{E}_{q(z|\\eta)} \\left[ \\log P(z_k | z_{-k}, x) \\right]\n",
    "    - \\mathbb{E}_{ q(z_k|\\eta_k) } \\left[ \\log q(z_k|\\eta_k) \\right] \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>At this point, we'll make the assumption that <mathjax>$P(z_k|z_{-k},x)$</mathjax> is an\n",
    "exponential family distribution (<mathjax>$z_{-k}$</mathjax> is all <mathjax>$z_i$</mathjax> with <mathjax>$i \\ne k$</mathjax>), and\n",
    "moreover that <mathjax>$q(z_k|\\eta_k)$</mathjax> and <mathjax>$P(z_k|z_{-k},x)$</mathjax> lie in the same exponential\n",
    "family.  Mathematically, this means that,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  q(z_k|\\eta_k)\n",
    "  &amp;= h(z_k) \\exp( \\eta_i^T t(z_k) - A(\\eta_k) \\\\\n",
    "  P(z_k|z_{-k},x)\n",
    "  &amp;= h(z_k) \\exp( g(z_{-k},x)^T t(z_k) - A(g(z_{-k},x)) \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Here <mathjax>$t(\\cdot)$</mathjax> are sufficient statistics, <mathjax>$A(\\cdot)$</mathjax> is the log of the\n",
    "normalizing constant, <mathjax>$g(\\cdot)$</mathjax> is a function of all other variables that\n",
    "determines the parameters for <mathjax>$P(z_k|z_{-k},x)$</mathjax>, and <mathjax>$h(\\cdot)$</mathjax> is some\n",
    "function that doesn't depend on the parameters of the distribution.</p>\n",
    "<p>Plugging this back into the previous equation (we define it to be\n",
    "<mathjax>$L(\\eta_k)$</mathjax>), applying the <mathjax>$\\log$</mathjax>, and using the linearity property of the\n",
    "expectation,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "&amp; \\arg\\max_{\\eta_k} &amp;&amp; L(\\eta_k) \\\\\n",
    "= &amp; \\arg\\max_{\\eta_k} &amp;&amp; \\mathbb{E}_{q(z|\\eta)} \\left[ \\log P(z_k | z_{-k}, x) \\right]\n",
    "    - \\mathbb{E}_{ q(z_k|\\eta_k) } \\left[ \\log q(z_k|\\eta_k) \\right] \\\\\n",
    "= &amp; \\arg\\max_{\\eta_k} &amp;&amp;\\mathbb{E}_{q(z|\\eta)} \\left[\n",
    "    \\log h(z_k) + g(z_{-k},x)^T t(z_k) - A(g(z_{-k},x)\n",
    "  \\right]\n",
    "  - \\mathbb{E}_{q(z_k|\\eta_k)} \\left[ \\log q(z_k|\\eta_k) \\right]  \\\\\n",
    "= &amp; \\arg\\max_{\\eta_k} &amp;&amp;\\left(\n",
    "    \\underbrace{ \\mathbb{E}_{q(z_k|\\eta_k)} \\left[ \\log h(z_k) \\right] }_{\\text{cancels out}}\n",
    "    + \\underbrace{\n",
    "      \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right]^T \\mathbb{E}_{q(z_{k}|\\eta_{k})} \\left[ t(z_k) \\right]\n",
    "    }_{\\text{$\\mathbb{E}$ splits b/c $q(z_{-k}|\\eta_{-k})$ and $q(z_k|\\eta_k)$ are indep.}}\n",
    "    - \\underbrace{ \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ A(g(z_{-k},x) \\right] }_{\\text{const wrt $\\eta_k$}}\n",
    "  \\right) \\\\\n",
    "&amp;&amp;&amp; - \\left(\n",
    "    \\underbrace{ \\mathbb{E}_{q(z_k|\\eta_k)} \\left[ \\log h(z_k) \\right] }_{\\text{cancels out}}\n",
    "    + \\eta_k^T \\mathbb{E}_{q(z_k|\\eta_k)} \\left[ t(z_k) \\right]\n",
    "    - A(\\eta_k)\n",
    "  \\right) \\\\\n",
    "= &amp; \\arg\\max_{\\eta_k} &amp;&amp; \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right]^T \\left( \\nabla_{\\eta_k} A(\\eta_k) \\right)\n",
    "    + \\eta_k^T \\left( \\nabla_{\\eta_k} A(\\eta_k) \\right)\n",
    "    - A(\\eta_k) \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>On this last line, we use the property <mathjax>$\\nabla A_{\\eta_k} (\\eta_k) =\n",
    "\\mathbb{E}_{q(z_k|\\eta_k)} [ t(z_k) ]$</mathjax>, a fact that holds for the exponential\n",
    "family.  Finally, let's take the gradient of this expression and set it to\n",
    "zero to solve for <mathjax>$\\eta_k$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  0\n",
    "  &amp; = \\nabla_{\\eta_k} L(\\eta_k) \\\\\n",
    "  &amp; = \\left( \\nabla_{\\eta_k}^2 A(\\eta_k) \\right)\n",
    "    \\left(\n",
    "      \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right]\n",
    "      - \\eta_k\n",
    "    \\right) \\\\\n",
    "  \\eta_k\n",
    "  &amp; = \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right] \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>So what is this expression? It says that in order to update <mathjax>$\\eta_k$</mathjax>, we need\n",
    "to be able to evaluate the expected parameters for <mathjax>$P(z_k|z_{-k},x)$</mathjax>\n",
    "under our approximation to the posterior <mathjax>$q(z_{-k}|\\eta_{-k})$</mathjax>. How do we do\n",
    "this? Let's take a look at an example to make this concrete.</p>\n",
    "<h1><a name=\"example\" href=\"#example\">Example</a></h1>\n",
    "<p>For this part, let's take a look at the model defined by Latent Dirichlet\n",
    "Allocation (LDA),</p>\n",
    "<div class=\"img-center\" style=\"max-width: 200px;\">\n",
    "  <img src=\"/assets/img/variational_inference/graphical_model.png\"></img>\n",
    "</div>\n",
    "\n",
    "<div class=\"pseudocode\">\n",
    "<p><strong>Input:</strong> document-topic prior <mathjax>$\\alpha$</mathjax>, topic-word prior <mathjax>$\\beta$</mathjax></p>\n",
    "<ol>\n",
    "<li>For each topic <mathjax>$k = 1 \\ldots K$</mathjax><ol>\n",
    "<li>Sample topic-word parameters <mathjax>$\\phi_{k} \\sim \\text{Dirichlet}(\\beta)$</mathjax></li>\n",
    "</ol>\n",
    "</li>\n",
    "<li>For each document <mathjax>$i = 1 \\ldots M$</mathjax><ol>\n",
    "<li>Sample document-topic parameters <mathjax>$\\theta_i \\sim \\text{Dirichlet}(\\alpha)$</mathjax></li>\n",
    "<li>For each token <mathjax>$j = 1 \\ldots N$</mathjax><ol>\n",
    "<li>Sample topic <mathjax>$z_{i,j} \\sim \\text{Categorical}(\\theta_i)$</mathjax></li>\n",
    "<li>Sample word <mathjax>$x_{i,j} \\sim \\text{Categorical}(\\phi_{z_{i,j}})$</mathjax></li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</div>\n",
    "<p>First, a short word on notation. In the following I'll occasionally drop\n",
    "indices to denote all variables with the same prefix. For example, when I say\n",
    "<mathjax>$\\theta$</mathjax>, I mean <mathjax>$\\theta_{1:M}$</mathjax>, and when I say <mathjax>$z_i$</mathjax>, I mean <mathjax>$z_{i,1:N}$</mathjax>.\n",
    "I'll also refer to <mathjax>$q(\\theta_i|\\eta_i)$</mathjax> as \"the variational distribution\n",
    "corresponding to <mathjax>$P(\\theta_i|\\alpha,\\theta_{-i},z,x)$</mathjax>\", and similarly for\n",
    "<mathjax>$q(z_{i,j}|\\gamma_{i,j})$</mathjax>. Oh, and <mathjax>$z_{-i}$</mathjax> means all <mathjax>$z_j$</mathjax> with <mathjax>$j \\ne i$</mathjax>, and\n",
    "<mathjax>$\\theta_{1:M}$</mathjax> means <mathjax>$(\\theta_1, \\ldots \\theta_M)$</mathjax>.</p>\n",
    "<p>Our goal now is to derive the posterior distribution over the latent\n",
    "variables, given the hyperparameters and the observed variables,\n",
    "<mathjax>$P(\\theta, z, \\phi| \\alpha, x, \\beta)$</mathjax>. We'll approximate it via the mean field\n",
    "distribution,</p>\n",
    "<p><mathjax>$$\n",
    "  q(\\theta,z,\\phi | \\eta,\\gamma,\\psi) = \\left(\n",
    "    \\prod_{i} q(\\theta_i | \\eta_i) \\prod_{j} q(z_{i,j} | \\gamma_{i,j})\n",
    "  \\right) \\left(\n",
    "    \\prod_{k} q(\\phi_k | \\psi_k)\n",
    "  \\right)\n",
    "$$</mathjax></p>\n",
    "<p><strong>Outline</strong> Deriving the update rules for Variational Inference requires we\n",
    "do 3 things. First, we must derive the posterior distribution for each hidden\n",
    "variable given all other variables, hidden and observed. This distribution must\n",
    "lie in the exponential family, and the corresponding variational distribution for\n",
    "that variable must be of the same form. For example, if\n",
    "<mathjax>$P(\\theta_i|\\alpha,\\theta_{-i},z,x)$</mathjax> is a Dirichlet distribution, then\n",
    "<mathjax>$q(\\theta_i|\\eta_i)$</mathjax> must also be Dirichlet.</p>\n",
    "<p>Second, we need to derive, for each hidden variable, the function that gives\n",
    "us the parameters for the posterior distribution over that variable given all\n",
    "others, hidden and observed.</p>\n",
    "<p>Finally, we'll need to plug the functions we just derived into an expectation\n",
    "with respect to the mean field distribution. If we are able to calculate this\n",
    "expectation for a particular hidden variable, we can use it to update the\n",
    "matching variational distribution's parameters.</p>\n",
    "<p>In the following, I'll show you how to derive the update for the variational\n",
    "distribution of one of the hidden variables in LDA, <mathjax>$\\theta_i$</mathjax>.</p>\n",
    "<p><strong>Step 1</strong> First, we must show that the posterior distribution over each\n",
    "individual hidden variable lies in the exponential family. This is not always\n",
    "the case, but for models that employ <a href=\"http://lesswrong.com/lw/5sn/the_joys_of_conjugate_priors/\">conjugate priors</a>, this\n",
    "can be guaranteed. A conjugate prior dictates that if <mathjax>$P(z)$</mathjax> is a conjugate\n",
    "prior to <mathjax>$P(x|z)$</mathjax>, then <mathjax>$P(z|x)$</mathjax> is in the same family as <mathjax>$P(z)$</mathjax> is. This is\n",
    "the case for Dirichlet/Categorical distributions such as those that appear in\n",
    "LDA. In other words, <mathjax>$P(\\theta_i|\\alpha,\\theta_{-i},z,x) =\n",
    "P(\\theta_i|\\alpha,z_{i})$</mathjax> (by conditional independence) is a Dirichlet\n",
    "distribution because <mathjax>$P(\\theta_i|\\alpha)$</mathjax> is Dirichlet and\n",
    "<mathjax>$P(z_{i,j}|\\theta_i)$</mathjax> is Categorical.</p>\n",
    "<p><strong>Step 2</strong> Next, we derive the parameter function for each hidden variable\n",
    "as a function of all other variables, hidden and observed. Let's see how this\n",
    "plays out for the Dirichlet distribution,</p>\n",
    "<p>The exponential family form of the Dirichlet distribution is,</p>\n",
    "<p><mathjax>$$\n",
    "  P(\\theta_i|\\alpha) = \\exp \\left(\n",
    "    \\sum_{k} (\\alpha_k - 1) \\log (\\theta_i)_k\n",
    "    - \\log \\left(\n",
    "      \\frac{ \\prod_{k} \\Gamma(\\alpha_k) }{ \\Gamma( \\sum_k \\alpha_k ) }\n",
    "    \\right)\n",
    "  \\right)\n",
    "$$</mathjax></p>\n",
    "<p>The exponential family form of a Categorical distribution is,</p>\n",
    "<p><mathjax>$$\n",
    "  P(z_{i,j}|\\theta_i) = \\exp \\left(\n",
    "    \\sum_{k} 1[z_{i,j} = k] \\log (\\theta_i)_k\n",
    "  \\right)\n",
    "$$</mathjax></p>\n",
    "<p>Thus, the posterior distribution for <mathjax>$\\theta_i$</mathjax> is proportional to,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  P(\\theta_i|\\alpha,z_{i})\n",
    "  &amp; \\propto P(\\theta_i, z_i | \\alpha) \\\\\n",
    "  &amp; = P(\\theta_i|\\alpha) \\prod_{j} P(z_{i,j}|\\theta_i) \\\\\n",
    "  &amp; = \\exp \\left(\n",
    "    \\sum_{k} \\left(\\alpha_k - 1 + \\sum_{j} 1[z_{i,j} = k] \\right) \\log (\\theta_i)_k\n",
    "  \\right)\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Notice how <mathjax>$\\alpha_k - 1$</mathjax> changed to <mathjax>$\\alpha_k - 1 + \\sum_{j} 1[z_{i,j} = k]$</mathjax>?\n",
    "These are the parameters for our posterior distribution over <mathjax>$\\theta_i$</mathjax>. Thus,\n",
    "the parameters for <mathjax>$P(\\theta_i|\\alpha,z_i)$</mathjax> are,</p>\n",
    "<p><mathjax>$$\n",
    "  g_{\\theta_i}(\\alpha,z_{i}) = \\begin{pmatrix}\n",
    "    \\alpha_1 - 1 + \\sum_{j} 1[z_{i,j} = 1] \\\\\n",
    "    \\alpha_2 - 1 + \\sum_{j} 1[z_{i,j} = 2] \\\\\n",
    "    \\vdots \\\\\n",
    "  \\end{pmatrix}\n",
    "$$</mathjax></p>\n",
    "<p><strong>Step 3</strong> Now we need to take the expectation over the parameter\n",
    "function we just derived with respect to the mean field distribution. For\n",
    "<mathjax>$g_{\\theta_i}(\\alpha, z_i)$</mathjax>, this is particularly easy -- all the indicators\n",
    "simply turn into probabilities. Thus the update for <mathjax>$q(\\theta_i|\\eta_i)$</mathjax> is,</p>\n",
    "<p><mathjax>$$\n",
    "  \\eta_i\n",
    "  = E_{q(z_{i}|\\gamma_i)} [ g_{\\theta_i}(\\alpha, z_i) ]\n",
    "  = \\begin{pmatrix}\n",
    "    \\alpha_1 - 1 + \\sum_{j} q(z_{i,j} = 1 | \\gamma_{i,j}) \\\\\n",
    "    \\alpha_2 - 1 + \\sum_{j} q(z_{i,j} = 2 | \\gamma_{i,j}) \\\\\n",
    "    \\vdots \\\\\n",
    "  \\end{pmatrix}\n",
    "$$</mathjax></p>\n",
    "<p><strong>Conclusion</strong> We've now derived the update rule for one of the components of\n",
    "the mean field distribution, <mathjax>$q(\\theta_i|\\eta_i)$</mathjax>. Left unexplained here is the\n",
    "updates for <mathjax>$q(z_{i,j}|\\gamma_{i,j})$</mathjax> and <mathjax>$q(\\phi_k|\\psi_k)$</mathjax>, though you can\n",
    "find a (messier) derivation in the original paper on <a href=\"http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf\">Latent Dirichlet\n",
    "Allocation</a>.</p>\n",
    "<h1><a name=\"aside\" href=\"#aside\">Aside: Coordinate Ascent is Gradient Ascent</a></h1>\n",
    "<p>Coordinate Ascent on the Mean Field Approximation is the \"traditional\" way\n",
    "one does Variational Inference, but Coordinate Ascent is far from the only\n",
    "optimization method we know. What if we wanted to do Gradient Ascent? What\n",
    "would an update look like then?</p>\n",
    "<p>It ends up that for the Variational Inference objective, Coordinate Ascent\n",
    "<em>is</em> Gradient Ascent with step size equal to 1. Actually, that's only half true\n",
    "-- it's Gradient Ascent using a \"Natural Gradient\" (rather than the usual\n",
    "gradient defined with respect to <mathjax>$||\\cdot||_2^2$</mathjax>).</p>\n",
    "<p><strong>Gradient Ascent</strong> First, recall the Gradient Ascent update for <mathjax>$\\eta_k$</mathjax> (we\n",
    "use the definition of <mathjax>$\\nabla_{\\eta_k} L(\\eta_k)$</mathjax> we found when deriving the\n",
    "Coordinate Ascent update).</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\eta_k^{(t+1)}\n",
    "  &amp; = \\eta_k^{(t)} + \\alpha^{(t)} \\nabla_{\\eta_k} L(\\eta_k^{(t)}) \\\\\n",
    "  &amp; = \\eta_k^{(t)} + \\alpha^{(t)} \\left[\n",
    "      \\left( \\nabla_{\\eta_k}^2 A(\\eta_k^{(t)}) \\right)\n",
    "      \\left(\n",
    "        \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right]\n",
    "        - \\eta_k^{(t)}\n",
    "      \\right)\n",
    "    \\right] \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p><strong>Natural Gradient</strong> Hmm, that <mathjax>$\\nabla_{\\eta_k}^2 A(\\eta_k^{(t)})$</mathjax> term is a\n",
    "bit of a nuisance. Is there any way to make it just go away? In fact, we can --\n",
    "by replacing the concept of a gradient with a \"natural gradient\". Whereas a\n",
    "regular gradient is the direction of steepest ascent with respect to Euclidean\n",
    "distance, a natural gradient is a direction of steepest ascent with respect to\n",
    "a function (in particular, one we want to minimize). The intuition is that for\n",
    "a given function, some input coordinates might be more important than others,\n",
    "and this should be taken into account when considering how far away 2 points\n",
    "are.</p>\n",
    "<p>So what do I mean \"a direction of steepest ascent\"?  Let's look at the\n",
    "gradient of a function as the solution to the following problem as <mathjax>$\\epsilon\n",
    "\\rightarrow 0$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\nabla_{\\eta_k} L(\\eta_k)\n",
    "  = &amp; \\arg\\min_{d \\eta_k} L(\\eta_k + d \\eta_k) \\\\\n",
    "    &amp; \\text{s.t.} \\quad   ||d \\eta_k||_2^2 &lt; \\epsilon\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>A natural gradient with respect to <mathjax>$L(\\eta_k)$</mathjax> is defined much the same way,\n",
    "but with <mathjax>$D_{E}(x,y) = || x-y ||_2^2$</mathjax> replaced with another squared metric.\n",
    "In our case, we're going to use the symmetrized KL divergence,</p>\n",
    "<p><mathjax>$$\n",
    "  D_{KL}(\\eta_k, \\eta_k') = \\text{KL} \\left[ q(z_k|\\eta_k) || q(z_k|\\eta_k') \\right]\n",
    "                          + \\text{KL} \\left[ q(z_k|\\eta_k') || q(z_k|\\eta_k) \\right]\n",
    "$$</mathjax></p>\n",
    "<p>Swapping the squared Euclidean metric <mathjax>$D_{E}$</mathjax> with <mathjax>$D_{KL}$</mathjax>, we have a\n",
    "definition for a \"Natural Gradient\",</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\hat{\\nabla}_{\\eta_k} L(\\eta_k)\n",
    "  = &amp; \\arg\\min_{d \\eta_k}   L(\\eta_k + d \\eta_k) \\\\\n",
    "    &amp; \\text{s.t.} \\quad D_{KL}(\\eta_k, \\eta_k + d \\eta_k) &lt; \\epsilon\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>While at first the gradient and natural gradient may seem difficult to\n",
    "relate, suppose that <mathjax>$D_{KL}(\\eta_k, \\eta_k + d \\eta_k) = d \\eta_k^T\n",
    "G(\\eta_k) d \\eta_k$</mathjax> for some matrix <mathjax>$G(\\eta_k)$</mathjax>. Then by plugging this into the\n",
    "previous optimization problem, replacing <mathjax>$L(\\eta_k + d \\eta_k)$</mathjax> by its first\n",
    "order Taylor approximation (which holds when <mathjax>$\\epsilon$</mathjax> is small), and\n",
    "requiring the derivative of the problem's Lagrangian be equal to 0, we see\n",
    "that,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  0\n",
    "  &amp; = \\nabla_{d \\eta_k} \\left[\n",
    "      L(\\eta_k + d \\eta_k) + \\lambda ( d \\eta_k G(\\eta_k) d \\eta_k - \\epsilon)\n",
    "    \\right] \\\\\n",
    "  &amp; \\approx \\nabla_{d \\eta_k} \\left[\n",
    "      L(\\eta_k) + \\nabla_{\\eta_k} L(\\eta_k)^T (\\eta_k + d \\eta_k - \\eta_k) + \\lambda ( d \\eta_k G(\\eta_k) d \\eta_k - \\epsilon)\n",
    "    \\right] \\\\\n",
    "  &amp; = \\nabla_{\\eta_k} L(\\eta_k) + 2 \\lambda G(\\eta_k) d \\eta_k \\\\\n",
    "  d \\eta_k\n",
    "  &amp; \\propto G(\\eta_k)^{-1} \\nabla_{\\eta_k} L(\\eta_k)\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>As <mathjax>$\\epsilon \\rightarrow 0$</mathjax>, <mathjax>$d \\eta_k$</mathjax> becomes <mathjax>$\\hat{\\nabla}_{\\eta_k}\n",
    "L(\\eta_k)$</mathjax>, resulting in <mathjax>$\\hat{\\nabla}_{\\eta_k} L(\\eta_k) \\propto\n",
    "G(\\eta_k)^{-1} \\nabla_{\\eta_k} L(\\eta_k)$</mathjax>. In other words, we can obtain\n",
    "<mathjax>$\\hat{\\nabla}_{\\eta_k} L(\\eta_k)$</mathjax> easily if we can simply compute <mathjax>$G(\\eta_k)$</mathjax>.\n",
    "Now let's derive <mathjax>$G(\\eta_k)$</mathjax>.</p>\n",
    "<p>First, let's take the first-order Taylor approximation to <mathjax>$q(z|\\eta_k + d\n",
    "\\eta_k)$</mathjax> and its <mathjax>$\\log$</mathjax> about <mathjax>$\\eta_k$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  q(z_k|\\eta_k + d \\eta_k)\n",
    "  &amp; \\approx q(z_k|\\eta_k) + (\\nabla q(z_k|\\eta_k))^T (\\eta_k + d \\eta_k - \\eta_k) \\\\\n",
    "  &amp; = q(z_k|\\eta_k) + q(z_k|\\eta_k) (\\nabla \\log q(z_k|\\eta_k))^T d \\eta_k \\\\\n",
    "  \\log q(z_k|\\eta_k + d \\eta_k)\n",
    "  &amp; \\approx \\log q(z_k|\\eta_k) + (\\nabla \\log q(z_k|\\eta_k))^T (\\eta_k + d \\eta_k - \\eta_k) \\\\\n",
    "  &amp; = \\log q(z_k|\\eta_k) + (\\nabla \\log q(z_k|\\eta_k))^T d \\eta_k \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Plugging this back into the definition of <mathjax>$D_{KL}$</mathjax> and cancelling out terms, we\n",
    "get a nice expression for <mathjax>$G(\\eta_k)$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  D_{KL}(\\eta, \\eta')\n",
    "  &amp; = \\text{KL} \\left[ q(z_k|\\eta_k) || q(z_k|\\eta_k + d\\eta_k) \\right] + \\text{KL} \\left[ q(z_k|\\eta_k + d\\eta_k) || q(z_k|\\eta_k) \\right] \\\\\n",
    "  &amp; = \\sum_{z} q(z|\\eta_k) \\log \\frac{ q(z|\\eta_k) }{ q(z|\\eta_k+d\\eta_k) }\n",
    "      + \\sum_{z} q(z|\\eta_k+d\\eta_k) \\log \\frac{ q(z|\\eta_k+d\\eta_k) }{ q(z|\\eta_k) } \\\\\n",
    "  &amp; = \\sum_{z} \\left[ q(z|\\eta_k) - q(z|\\eta_k+d\\eta_k) \\right]\n",
    "               \\left[ \\log q(z|\\eta_k) - \\log q(z|\\eta_k+d\\eta_k) \\right] \\\\\n",
    "  &amp; \\approx \\sum_{z}\n",
    "      \\left[ q(z|\\eta_k) - q(z_k|\\eta_k) - q(z_k|\\eta_k)(\\nabla \\log q(z_k|\\eta_k))^T d \\eta_k \\right] \\\\\n",
    "      &amp; \\qquad \\quad \\times \\left[ \\log q(z|\\eta_k) - \\log q(z_k|\\eta_k) - (\\nabla \\log q(z_k|\\eta_k))^T d \\eta_k \\right] \\\\\n",
    "  &amp; = \\sum_{z}\n",
    "      \\left[ - q(z_k|\\eta_k) (\\nabla \\log q(z_k|\\eta_k))^T d \\eta_k \\right]\n",
    "      \\left[ - (\\nabla \\log q(z_k|\\eta_k))^T d \\eta_k \\right] \\\\\n",
    "  &amp; = d \\eta_k^T \\mathbb{E}_{q(z|\\eta_k)} \\left[ (\\nabla \\log q(z_k|\\eta_k)) (\\nabla \\log q(z_k|\\eta_k))^T \\right] d \\eta_k \\\\\n",
    "  &amp; = d \\eta_k^T G(\\eta_k) d \\eta_k \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Looking at the expression for <mathjax>$G(\\eta_k)$</mathjax>, we can see that it is in fact the\n",
    "<a href=\"http://en.wikipedia.org/wiki/Fisher_information\">Fisher Information Matrix</a>. Since we already assumed that\n",
    "<mathjax>$q(z_k|\\eta_k)$</mathjax> is in the exponential family, let's plug in its exponential\n",
    "form <mathjax>$q(z_k|\\eta_k) = h(z_k) \\exp \\left( \\eta_k^T t(z_k) - A(\\eta_k) \\right)$</mathjax>\n",
    "and apply the <mathjax>$\\log$</mathjax> to see that we are simply taking the covariance matrix of\n",
    "the sufficient statistics <mathjax>$t(z_k)$</mathjax>. For exponential families, this also happens\n",
    "to be the second derivative of the log normalizing constant,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  G(\\eta_k)\n",
    "  &amp;= \\mathbb{E}_{q(z_k|\\eta_k)} \\left[\n",
    "      \\left( \\nabla_{\\eta_k} \\log q(z_k|\\eta_k) \\right) \\left( \\nabla_{\\eta_k} \\log q(z_k|\\eta_k) \\right)^T\n",
    "    \\right] \\\\\n",
    "  &amp;= \\mathbb{E}_{q(z_k|\\eta_k)} \\left[\n",
    "      \\left( t(z_k) - \\nabla_{\\eta_k} A(\\eta_k) \\right) \\left( t(z_k) - \\nabla_{\\eta_k} A(\\eta_k) \\right)^T\n",
    "    \\right] \\\\\n",
    "  &amp;= \\mathbb{E}_{q(z_k|\\eta_k)} \\left[\n",
    "      \\left( t(z_k) - \\mathbb{E}_{q(z_k|\\eta_k)} [t(z_k)] \\right) \\left( t(z_k) - \\mathbb{E}_{q(z_k|\\eta_k)} [t(z_k)] \\right)^T\n",
    "    \\right] \\\\\n",
    "  &amp;= \\nabla_{\\eta_k}^2 A(\\eta_k) \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Finally, let's define a Gradient Ascent algorithm in terms of the Natural\n",
    "Gradient, rather than the regular gradient,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\eta_k^{(t+1)}\n",
    "  &amp; = \\eta_k^{(t)} + \\alpha^{(t)} \\hat{\\nabla}_{\\eta_k} L(\\eta_k^{(t)}) \\\\\n",
    "  &amp; = \\eta_k^{(t)} + \\alpha^{(t)} G(\\eta_k^{(t)})^{-1} \\nabla_{\\eta_k} L(\\eta_k^{(t)}) \\\\\n",
    "  &amp; = \\eta_k^{(t)} + \\alpha^{(t)}\n",
    "    \\left( \\nabla_{\\eta_k}^2 A(\\eta_k^{(t)}) \\right)^{-1}\n",
    "    \\left[\n",
    "      \\left( \\nabla_{\\eta_k}^2 A(\\eta_k^{(t)}) \\right)\n",
    "      \\left(\n",
    "        \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right]\n",
    "        - \\eta_k^{(t)}\n",
    "      \\right)\n",
    "    \\right] \\\\\n",
    "  &amp; = (1 - \\alpha^{(t)}) \\eta_k^{(t)}\n",
    "    + \\alpha^{(t)} \\mathbb{E}_{q(z_{-k}|\\eta_{-k})} \\left[ g(z_{-k},x) \\right] \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Look at that -- <mathjax>$G(\\eta_k^{(t)})^{-1} = (\\nabla_{\\eta_k}^2 A(\\eta_k))^{-1}$</mathjax>\n",
    "perfectly cancels out <mathjax>$\\nabla_{\\eta_k}^2 A(\\eta_k)$</mathjax>, and we're left with a\n",
    "linear combination of the old parameters and the parameters Coordinate Ascent\n",
    "would recommend. If <mathjax>$\\alpha^{(t)} = 1$</mathjax>, then we just get the old Coordinate\n",
    "Ascent update!</p>\n",
    "<h1><a name=\"extensions\" href=\"#extensions\">Extensions</a></h1>\n",
    "<p>The Variational Inference method I described here, while general in concept,\n",
    "can only easily be applied to a very particular class models -- ones where\n",
    "<mathjax>$P(z_k | z_{-k}, x)$</mathjax> is in the exponential family. This more or less means that\n",
    "<mathjax>$z_k$</mathjax> be a discrete variable or that <mathjax>$P(z_k)$</mathjax> be a conjugate prior to all other\n",
    "variables depending on it.</p>\n",
    "<p>In addition, we restricted <mathjax>$q(z | \\eta)$</mathjax> to be a mean field approximation,\n",
    "meaning that each variable is independent with its own distribution <mathjax>$q(z_k |\n",
    "\\eta_k)$</mathjax>. This approximation has no hope of representing any interactions\n",
    "between variables, and perhaps surprisingly <mathjax>$q(z_k|\\eta_k)$</mathjax> does <em>not match the\n",
    "marginal distribution over <mathjax>$z_k$</mathjax> at all.</em>  This is a common source of confusion\n",
    "for first-time users, and makes debugging Variational Inference algorithms\n",
    "rather difficult.</p>\n",
    "<p>Third, the Coordinate Ascent algorithm described is not necessarily quick. I\n",
    "explained how Coordinate Ascent is really just Gradient Ascent on the natural\n",
    "gradient, so it's easy to ask what other methods we might be able to apply.</p>\n",
    "<p>Here are a handful of papers that extend Variational Inference to faster\n",
    "optimization methods, different variational distribution, and non-conjugate\n",
    "models.</p>\n",
    "<p><a href=\"http://books.nips.cc/papers/files/nips25/NIPS2012_1314.pdf\">\"Fast Variational Inference in the Conjugate Exponential\n",
    "Family\"</a> -- Conjugate Gradient applied to the Marginalized\n",
    "Variational Bound. Shows that the Marginalized Variational Bound upper bounds the\n",
    "typical Variational Bound and that the former also has better curvature. That\n",
    "means second-order optimizers like Conjugate Gradient can take larger steps and\n",
    "render better performance.</p>\n",
    "<p><a href=\"http://arxiv.org/abs/1206.6679\">\"Fixed-Form Variational Posterior Approximation through Stochastic Linear\n",
    "Regression\"</a> -- fits a (potentially) non-decomposable\n",
    "exponential family distribution via Linear Regression. Involves looking at KL\n",
    "divergence between unnormalized variational distribution and joint distribution\n",
    "of model, taking derivative with respect to variational distribution's\n",
    "parameters and setting to 0, then solving for the parameters. Can be applied to\n",
    "non-conjugate models due to sampling for estimating expectations.</p>\n",
    "<p><a href=\"http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf\">\"Variational Inference in Nonconjugate Models\"</a> -- Getting\n",
    "away from conjugate priors via Laplace and the Delta Method.</p>\n",
    "<h1><a name=\"references\" href=\"#references\">References</a></h1>\n",
    "<p>The seminal work on the Natural Gradient is due to Shunichi Amari's <a href=\"http://www.maths.tcd.ie/~mnl/store/Amari1998a.pdf\">\"Natural\n",
    "Gradient Works Efficiently in Learning\"</a>. The derivation for the natural\n",
    "gradient is Theorem 1. Thanks to <a href=\"https://twitter.com/atpassos_ml\">Alexandre Passos</a> for suggesting\n",
    "this and giving a short-hand intuition of the proof.</p>\n",
    "<p>The derivation for Variational Inference and the correspondence between\n",
    "Coordinate Ascent and Gradient Ascent is based on the introduction to Matt\n",
    "Hoffman et al.'s <a href=\"http://arxiv.org/abs/1206.7051\">\"Stochastic Variational Inference\"</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
