{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>Convex clustering is the reformulation of k-means clustering as a convex\n",
    "problem. While the two problems are not equivalent, the former can be seen as a\n",
    "relaxation of the latter that allows us to easily find globally optimal\n",
    "solutions (as opposed to only locally optimal ones).</p>\n",
    "<p>Suppose we have a set of points <mathjax>$\\{ x_i : i = 1, \\ldots, n\\}$</mathjax>. Our goal is to\n",
    "partition these points into groups such that all the elements in each group are\n",
    "close to each other and are distant from points in other groups.</p>\n",
    "<p>In this post, I'll talk about an algorithm to do just that.</p>\n",
    "<h1><a name=\"k-means\" href=\"#k-means\">K-Means</a></h1>\n",
    "<p>The original objective for k-means clustering is as follows. Suppose we want\n",
    "to find <mathjax>$k$</mathjax> sets <mathjax>$S_i$</mathjax> such that every <mathjax>$x_i$</mathjax> is in exactly 1 set <mathjax>$S_j$</mathjax>. Each <mathjax>$S_j$</mathjax>\n",
    "will then have a center <mathjax>$\\theta_j$</mathjax>, which is simply the average of all <mathjax>$x_i$</mathjax> it\n",
    "contains. Putting it all together, we obtain the following optimization problme,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp \\underset{S}{\\min}  &amp; &amp; \\sum_{j=1}^{k} \\sum_{i \\in S_j} ||x_i - \\theta_j||_2^2 \\\\\n",
    "  &amp; \\text{subject to}   &amp; &amp; \\theta_j = \\frac{1}{|S_j|} \\sum_{i \\in S_j} x_i \\\\\n",
    "  &amp;                     &amp; &amp; \\bigcup_{j} S_j = \\{ 1 \\ldots n \\}\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>In 2009, <a href=\"http://dl.acm.org/citation.cfm?id=1519389\">Aloise et al.</a> proved that solving this problem is\n",
    "NP-hard, meaning that short of enumerating every possible partition, we cannot\n",
    "say whether or not we've found an optimal solution <mathjax>$S^{*}$</mathjax>. In other words, we\n",
    "can approximately solve k-means, but actually solving it is very\n",
    "computationally intense (with the usual caveats about <mathjax>$P = NP$</mathjax>).</p>\n",
    "<h1><a name=\"convex-clustering\" href=\"#convex-clustering\">Convex Clustering</a></h1>\n",
    "<p>Convex clustering sidesteps this complexity result by proposing a new\n",
    "problem that we <em>can</em> solve quickly. The optimal solution for this new problem\n",
    "need not coincide with that of k-means, but <a href=\"http://www.control.isy.liu.se/research/reports/2011/2992.pdf\">can be seen</a> a solution to\n",
    "the convex relaxation of the original problem.</p>\n",
    "<p>The idea of convex clustering is that each point <mathjax>$x_i$</mathjax> is paired with its\n",
    "associated center <mathjax>$u_i$</mathjax>, and the distance between the two is minimized. If this\n",
    "were nothing else, <mathjax>$u_i = x_i$</mathjax> would be the optimal solution, and no\n",
    "clustering would happen. Instead, a penalty term is added that brings the\n",
    "clusters centers close together,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\min_{u} \\frac{1}{2} \\sum_{i=1}^{n} ||x_i - u_i||_2^2\n",
    "            + \\gamma \\sum_{i &lt; j} w_{i,j} ||u_i - u_j||_p\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Notice that the distance <mathjax>$||x_i - u_i||_2^2$</mathjax> is a squared 2-norm, but\n",
    "the distance between the centers <mathjax>$||u_i - u_j||_p$</mathjax> is a p-norm (<mathjax>$p \\in \\{1, 2,\n",
    "\\infty \\}$</mathjax>). This sum-of-norms type penalization brings about \"group sparsity\"\n",
    "and is used primarily because many of the elements in this sum will be 0 at the\n",
    "optimum. In convex clustering, that means <mathjax>$u_i = u_j$</mathjax> for some pairs <mathjax>$i$</mathjax> and\n",
    "<mathjax>$j$</mathjax> -- in other words, <mathjax>$i$</mathjax> and <mathjax>$j$</mathjax> are clustered together!</p>\n",
    "<h1><a name=\"algorithms\" href=\"#algorithms\">Algorithms for Convex Clustering</a></h1>\n",
    "<p>As the convex clustering formulation is a convex problem, we automatically\n",
    "get a variety of black-box algorithms capable of solving it. Unfortunately, the\n",
    "number of variables in the problem is rather large -- if <mathjax>$x_i \\in\n",
    "\\mathcal{R}^{d}$</mathjax>, then <mathjax>$u \\in \\mathcal{R}^{n \\times d}$</mathjax>.  If <mathjax>$d = 5$</mathjax>, we cannot\n",
    "reasonably expect interior point solvers such as <a href=\"http://cvxr.com/cvx/\">cvx</a> to handle any more\n",
    "than a few thousand points.</p>\n",
    "<p><a href=\"http://www.icml-2011.org/papers/419_icmlpaper.pdf\">Hocking et al.</a> and <a href=\"http://arxiv.org/abs/1304.0499\">Chi et al.</a> were the first to design\n",
    "algorithms specifically for convex clustering. The former designed one\n",
    "algorithm for each <mathjax>$p$</mathjax>-norm, employing active sets (<mathjax>$p \\in \\{1, 2\\}$</mathjax>),\n",
    "subgradient descent (<mathjax>$p = 2$</mathjax>), and the Frank-Wolfe algorithm (<mathjax>$p = \\infty$</mathjax>).\n",
    "The latter makes use of <a href=\"http://www.stanford.edu/~boyd/papers/admm_distr_stats.html\">ADMM</a> and AMA, the latter of which reduces to\n",
    "proximal gradient on a dual objective.</p>\n",
    "<p>Here, I'll describe another method for solving the convex clustering problem\n",
    "based on coordinate ascent. The idea is to take the original formulation,\n",
    "substitute a new primal variable <mathjax>$z_l = u_{l_1} - u_{l_2}$</mathjax>, then update a dual\n",
    "variable <mathjax>$\\lambda_l$</mathjax> corresponding to each equality constraint 1 at a time. For\n",
    "this problem, we can reconstruct the primal variables <mathjax>$u_i$</mathjax> in closed form\n",
    "given the dual variables, so it is easy to check how close we are to the\n",
    "optimum.</p>\n",
    "<!--\n",
    "  <table class=\"table table-hover table-bordered\">\n",
    "    <tr>\n",
    "      <th>Name</th>\n",
    "      <th>Memory required</th>\n",
    "      <th>per-iteration complexity</th>\n",
    "      <th>number of iterations required</th>\n",
    "      <th>parallelizability</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Clusterpath ($L_1$)</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>1</td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Clusterpath ($L_2$)</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Clusterpath ($L_{\\infty}$)</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>ADMM</td>\n",
    "      <td>$O(pd)$</td>\n",
    "      <td>$O(pd)$</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>AMA (accelerated)</td>\n",
    "      <td>$O(pd)$</td>\n",
    "      <td>$O(pd)$</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Coordinate Ascent</td>\n",
    "      <td>$O(pd)$</td>\n",
    "      <td>$O(pd)$</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "  </table>\n",
    "\n",
    "  For $p =$ number of pairs with $w_l > 0$, $n =$ the number of points $x_i$,\n",
    "$d =$ the dimensionality of $x_i$, $c = $ the current number of clusters\n",
    "-->\n",
    "\n",
    "<h1><a name=\"reformulation\" href=\"#reformulation\">Problem Reformulation</a></h1>\n",
    "<p>To describe the dual problem being maximized, we first need to modify the\n",
    "primal problem. First, let <mathjax>$z_l = u_{l_1} - u_{l_2}$</mathjax>. Then we can write the\n",
    "objective function as,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\underset{S}{\\min}  &amp; &amp; \\frac{1}{2} \\sum_{i=1}^{n} ||x_i - u_i||_2^2\n",
    "                            + \\gamma \\sum_{l} w_{l} ||z_l||_p \\\\\n",
    "  &amp; \\text{subject to}   &amp; &amp; z_l = u_{l_1} - u_{l_2}\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p><a href=\"http://arxiv.org/abs/1304.0499\">Chi et al.</a> show on page 6 that the dual of this problem is then,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\underset{\\lambda}{\\max}  &amp; &amp; - \\frac{1}{2} \\sum_{i} ||\\Delta_i||_2^2\n",
    "                                  - \\sum_{l} \\lambda_l^T (x_{l_1} - x_{l_2}) \\\\\n",
    "  &amp; \\text{subject to}         &amp; &amp; ||\\lambda_l||_{p^{*}} \\le \\gamma w_l \\\\\n",
    "  &amp;                           &amp; &amp; \\Delta_{i} = \\sum_{l: l_1 = i} \\lambda_l - \\sum_{l : l_2 = i} \\lambda_l\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>In this notation, <mathjax>$||\\cdot||_{p^{*}}$</mathjax> is the dual norm of <mathjax>$||\\cdot||_p$</mathjax>. The\n",
    "primal variables <mathjax>$u$</mathjax> and dual variables <mathjax>$\\lambda$</mathjax> are then related by the\n",
    "following equation,</p>\n",
    "<p><mathjax>$$\n",
    "  u_i = \\Delta_i + x_i\n",
    "$$</mathjax></p>\n",
    "<h1><a name=\"coordinate-ascent\" href=\"#coordinate-ascent\">Coordinate Ascent</a></h1>\n",
    "<p>Now let's optimize the dual problem 1 <mathjax>$\\lambda_k$</mathjax> at a time. First, notice\n",
    "that <mathjax>$\\lambda_k$</mathjax> will only appear in 2 <mathjax>$\\Delta_i$</mathjax> terms -- <mathjax>$\\Delta_{k_1}$</mathjax> and\n",
    "<mathjax>$\\Delta_{k_2}$</mathjax>. After dropping all terms independent of <mathjax>$\\lambda_k$</mathjax>, we now get\n",
    "the following problem,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\underset{\\lambda_k}{\\min}  &amp; &amp; \\frac{1}{2} (||\\Delta_{k_1}||_2^2 + ||\\Delta_{k_2}||_2^2)\n",
    "                                    + \\lambda_k^T (x_{k_1} - x_{k_2}) \\\\\n",
    "  &amp; \\text{subject to}         &amp; &amp; ||\\lambda_k||_{p^{*}} \\le \\gamma w_k \\\\\n",
    "  &amp;                           &amp; &amp; \\Delta_{k_1} = \\sum_{l: l_1 = k_1} \\lambda_l - \\sum_{l : l_2 = k_1} \\lambda_l \\\\\n",
    "  &amp;                           &amp; &amp; \\Delta_{k_2} = \\sum_{l: l_1 = k_2} \\lambda_l - \\sum_{l : l_2 = k_2} \\lambda_l\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>We can pull <mathjax>$\\lambda_k$</mathjax> out of <mathjax>$\\Delta_{k_1}$</mathjax> and <mathjax>$\\Delta_{k_2}$</mathjax> to get,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  ||\\Delta_{k_1}||_2^2 &amp; = ||\\lambda_k||_2^2 + ||\\Delta_{k_1} - \\lambda_k||_2^2 + 2 \\lambda_k^T (\\Delta_{k_1} - \\lambda_k) \\\\\n",
    "  ||\\Delta_{k_2}||_2^2 &amp; = ||\\lambda_k||_2^2 + ||\\Delta_{k_2} + \\lambda_k||_2^2 - 2 \\lambda_k^T (\\Delta_{k_2} + \\lambda_k)\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Let's define <mathjax>$\\tilde{\\Delta_{k_1}} = \\Delta_{k_1} - \\lambda_k$</mathjax> and\n",
    "<mathjax>$\\tilde{\\Delta_{k_2}} = \\Delta_{k_2} + \\lambda_k$</mathjax> and add <mathjax>$||\\frac{1}{2}\n",
    "(\\tilde{\\Delta_{k_1}} - \\tilde{\\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2$</mathjax> to the\n",
    "objective.</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\underset{\\lambda_k}{\\min}  &amp; &amp; ||\\lambda_k||_2^2\n",
    "                                    + 2 \\frac{1}{2} \\lambda_k^T (\\tilde{\\Delta_{k_1}} - \\tilde{\\Delta_{k_2}} + x_{k_1} - x_{k_2})\n",
    "                                    + ||\\frac{1}{2} (\\tilde{\\Delta_{k_1}} - \\tilde{\\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2 \\\\\n",
    "  &amp; \\text{subject to}         &amp; &amp; ||\\lambda_k||_{p^{*}} \\le \\gamma w_k \\\\\n",
    "  &amp;                           &amp; &amp; \\tilde{\\Delta_{k_1}} = \\sum_{l: l_1 = k_1; l \\ne k} \\lambda_l - \\sum_{l : l_2 = k_1; l \\ne k} \\lambda_l \\\\\n",
    "  &amp;                           &amp; &amp; \\tilde{\\Delta_{k_2}} = \\sum_{l: l_1 = k_2; l \\ne k} \\lambda_l - \\sum_{l : l_2 = k_2; l \\ne k} \\lambda_l\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>We can now factor the objective into a quadratic,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  &amp; \\underset{\\lambda_k}{\\min}  &amp; &amp; ||\\lambda_k - \\left( - \\frac{1}{2}(\\tilde{\\Delta_{k_1}} - \\tilde{\\Delta_{k_2}} + x_{k_1} - x_{k_2}) \\right) ||_2^2 \\\\\n",
    "  &amp; \\text{subject to}         &amp; &amp; ||\\lambda_k||_{p^{*}} \\le \\gamma w_k \\\\\n",
    "  &amp;                           &amp; &amp; \\tilde{\\Delta_{k_1}} = \\sum_{l: l_1 = k_1; l \\ne k} \\lambda_l - \\sum_{l : l_2 = k_1; l \\ne k} \\lambda_l \\\\\n",
    "  &amp;                           &amp; &amp; \\tilde{\\Delta_{k_2}} = \\sum_{l: l_1 = k_2; l \\ne k} \\lambda_l - \\sum_{l : l_2 = k_2; l \\ne k} \\lambda_l\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>This problem is simply a Euclidean projection onto the ball defined by\n",
    "<mathjax>$||\\cdot||_{p^{*}}$</mathjax>. We're now ready to write the algorithm,</p>\n",
    "<div class=\"pseudocode\">\n",
    "<p><strong>Input:</strong> Initial dual variables <mathjax>$\\lambda^{(0)}$</mathjax>, weights <mathjax>$w_l$</mathjax>, and regularization parameter <mathjax>$\\gamma$</mathjax></p>\n",
    "<ol>\n",
    "<li>Initialize <mathjax>$\\Delta_i^{(0)} = \\sum_{l: l_1 = i} \\lambda_l^{(0)} - \\sum_{l: l_2 = i} \\lambda_l^{(0)}$</mathjax></li>\n",
    "<li>For each iteration <mathjax>$m = 0,1,2,\\ldots$</mathjax> until convergence<ol>\n",
    "<li>Let <mathjax>$\\Delta^{(m+1)} = \\Delta^{(m)}$</mathjax></li>\n",
    "<li>For each pair of points <mathjax>$l = (i,j)$</mathjax> with <mathjax>$w_{l} &gt; 0$</mathjax><ol>\n",
    "<li>Let <mathjax>$\\Delta_i^{(m+1)} \\leftarrow \\Delta_i^{(m+1)} - \\lambda_l^{(m)}$</mathjax> and <mathjax>$\\Delta_j^{(m+1)} \\leftarrow \\Delta_i^{(m+1)} + \\lambda_l^{(m)}$</mathjax></li>\n",
    "<li><mathjax>$\\lambda_l^{(m+1)} = \\text{project}(- \\frac{1}{2}(\\Delta_i^{(m+1)} - \\Delta_j^{(m+1)} + x_{i} - x_{j}),\n",
    "                                       \\{ \\lambda : ||\\lambda||_{p^{*}} \\le \\gamma w_l \\}$</mathjax>)</li>\n",
    "<li><mathjax>$\\Delta_i^{(m+1)} \\leftarrow \\Delta_i^{(m+1)} + \\lambda_l^{(m+1)}$</mathjax> and <mathjax>$\\Delta_j^{(m+1)} \\leftarrow \\Delta_j^{(m+1)} - \\lambda_l^{(m+1)}$</mathjax></li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</li>\n",
    "<li>Return <mathjax>$u_i = \\Delta_i + x_i$</mathjax> for all <mathjax>$i$</mathjax></li>\n",
    "</ol>\n",
    "</div>\n",
    "<p>Since we can easily construct the primal variables from the dual variables\n",
    "and can evaluate the primal and dual functions in closed form, we can use the\n",
    "duality gap to determine when we are converged.</p>\n",
    "<h1><a name=\"conclusion\" href=\"#conclusion\">Conclusion</a></h1>\n",
    "<p>In this post, I introduced a coordinate ascent algorithm for convex\n",
    "clustering. Empirically, the algorithm is quite quick, but it doesn't share the\n",
    "parallelizability or convergence proofs of its siblings, ADMM and AMA. However,\n",
    "coordinate descent has an upside: there are no parameters to tune, and every\n",
    "iteration is guaranteed to improve the objective function. Within each\n",
    "iteration, updates are quick asymptotically and empirically.</p>\n",
    "<p>Unfortunately, like all algorithms based on the dual for this particular\n",
    "problem, the biggest burden is on memory. Whereas the primal formulation\n",
    "requires the number of variables grow linearly with the number of data points,\n",
    "the dual formulation can grow as high as quadratically. In addition, the primal\n",
    "variables allow for centers to be merged, allowing for potential space-savings\n",
    "as the algorithm is running. The dual seems to lack this property, requiring\n",
    "all dual variables to be fully instantiated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
