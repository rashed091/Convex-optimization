{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this post, we'll take a look at the <a href=\"http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm\">Frank-Wolfe Algorithm</a>\n",
    "also known as the Conditional Gradient Method, an algorithm particularly suited\n",
    "for solving problems with compact domains. Like the <a href=\"http://stronglyconvex.com/blog/proximal-gradient-descent.html\">Proximal\n",
    "Gradient</a> and <a href=\"http://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html\">Accelerated Proximal\n",
    "Gradient</a> algorithms, Frank-Wolfe requires we\n",
    "exploit problem structure to quickly solve a mini-optimization problem. Our\n",
    "reward for doing so is a converge rate of <mathjax>$O(1/\\epsilon)$</mathjax> and the potential for\n",
    "<em>extremely sparse solutions</em>.</p>\n",
    "<p>Returning to my <a href=\"http://stronglyconvex.com/blog/gradient-descent.html\">valley-finding metaphor</a>, Frank-Wolfe is a\n",
    "bit like this,</p>\n",
    "<div class=\"pseudocode\">\n",
    "<ol>\n",
    "<li>Look around you and see which way points the most downwards</li>\n",
    "<li>Walk as far as possible in that direction until you hit a wall</li>\n",
    "<li>Go back in the direction you started, stop part way along the path, then\n",
    "     repeat.</li>\n",
    "</ol>\n",
    "</div>\n",
    "<h1><a name=\"implementation\" href=\"#implementation\">How does it work?</a></h1>\n",
    "<p>Frank-Wolfe is designed to solve problems of the form,</p>\n",
    "<p><mathjax>$$\n",
    "  \\min_{x \\in D} f(x)\n",
    "$$</mathjax></p>\n",
    "<p>where <mathjax>$D$</mathjax> is compact and <mathjax>$f$</mathjax> is differentiable. For example, in <mathjax>$R^n$</mathjax> any\n",
    "closed and bounded set is compact. The algorithm for Frank-Wolfe is then,</p>\n",
    "<div class=\"pseudocode\">\n",
    "<p><strong>Input</strong>: Initial iterate <mathjax>$x^{(0)}$</mathjax></p>\n",
    "<ol>\n",
    "<li>For <mathjax>$t = 0, 1, 2, \\ldots$</mathjax><ol>\n",
    "<li>Let <mathjax>$s^{(t+1)} = \\arg\\min_{s \\in D} \\langle \\nabla f(x^{(t)}), s \\rangle$</mathjax></li>\n",
    "<li>If <mathjax>$g(x) = \\langle \\nabla f(x^{(t)}), x - s^{(t+1)} \\rangle \\le \\epsilon$</mathjax>, break</li>\n",
    "<li>Let <mathjax>$x^{(t+1)} = (1 - \\alpha^{(t)}) x^{(t)} + \\alpha^{(t)} s^{(t+1)}$</mathjax></li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</div>\n",
    "<p>The proof relies on <mathjax>$\\alpha^{(t)} = 2 / (t+2)$</mathjax>, but line search works as\n",
    "well.  The intuition for the algorithm is that at each iteration, we minimize\n",
    "a linear approximation to <mathjax>$f$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  s^{(t+1)} = \\arg\\min_{s \\in D} f(x^{(t)}) + \\nabla f(x^{(t)})^T (s - x^{(t)})\n",
    "$$</mathjax></p>\n",
    "<p>then take a step in that direction. We can immediately see that if <mathjax>$D$</mathjax>\n",
    "weren't compact, <mathjax>$s^{(t)}$</mathjax> would go off to infinity.</p>\n",
    "<p><a id=\"upper_bound\"></a>\n",
    "  <strong>Upper Bound</strong> One nice property of Frank-Wolfe is that it comes with its\n",
    "own upper bound on <mathjax>$f(x^{(t)}) - f(x^{*})$</mathjax> calculated during the course of\n",
    "the algorithm. Recall the linear upper bound on <mathjax>$f$</mathjax> due to convexity,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{*})\n",
    "  &amp; \\ge f(x) + \\nabla f(x)^T (x^{*} - x) \\\\\n",
    "  f(x) - f(x^{*})\n",
    "  &amp; \\le \\nabla f(x)^T (x - x^{*}) \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Since,</p>\n",
    "<p><mathjax>$$\n",
    "  s^{(t+1)}\n",
    "  = \\arg\\min_{s} \\nabla f(x^{(t)})^T s\n",
    "  = \\arg\\max_{s} \\nabla f(x^{(t)})^T (x^{(t)} - s)\n",
    "$$</mathjax>\n",
    "  we know that <mathjax>$\\nabla f(x^{(t)})^T (x^{(t)} - x^{*}) \\le \\nabla f(x^{(t)})^T\n",
    "(x^{(t)} - s^{(t+1)})$</mathjax> and thus,</p>\n",
    "<p><mathjax>$$\n",
    "  f(x) - f(x^{*}) \\le \\nabla f(x^{(t)})^T (x^{(t)} - s^{(t+1)})\n",
    "$$</mathjax></p>\n",
    "<p><a id=\"example\"></a></p>\n",
    "<h1><a name=\"example\" href=\"#example\">A Small Example</a></h1>\n",
    "<p>For this example, we'll minimize a simple univariate quadratic function\n",
    "constrained to lie in an interval,</p>\n",
    "<p><mathjax>$$\n",
    "  \\min_{x \\in [-1,2]} (x-0.5)^2 + 2x\n",
    "$$</mathjax></p>\n",
    "<p>Its derivative is given by <mathjax>$2(x-0.5) + 2$</mathjax>, and since we are dealing with real\n",
    "numbers, the minimizers of the linear approximation must be either <mathjax>$-1$</mathjax> or\n",
    "<mathjax>$2$</mathjax> if the gradient is positive or negative, respectively. We'll use a stepsize\n",
    "of <mathjax>$\\alpha^{(t)} = 2 / (t+2)$</mathjax> as prescribed by the convergence proof in the\n",
    "next section.</p>\n",
    "<div class=\"img-center\">\n",
    "  <img src=\"/assets/img/frank_wolfe/animation.gif\"></img>\n",
    "  <span class=\"caption\">\n",
    "    Frank-Wolfe in action. The red circle is the current value for\n",
    "    $f(x^{(t)})$, and the green diamond is $f(x^{(t+1)})$. The dotted line is\n",
    "    the linear approximation to $f$ at $x^{(t)}$. Notice that at each step,\n",
    "    Frank-Wolfe stays closer and closer to $x^{(t)}$ when moving in the\n",
    "    direction of $s^{(t+1)}$.\n",
    "  </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"img-center\">\n",
    "  <img src=\"/assets/img/frank_wolfe/convergence.png\"></img>\n",
    "  <span class=\"caption\">\n",
    "    This plot shows how quickly the objective function decreases as the\n",
    "    number of iterations increases. Notice that it does not monotonically\n",
    "    decrease, as with Gradient Descent.\n",
    "  </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"img-center\">\n",
    "  <img src=\"/assets/img/frank_wolfe/iterates.png\"></img>\n",
    "  <span class=\"caption\">\n",
    "    This plot shows the actual iterates and the objective function evaluated at\n",
    "    those points. More red indicates a higher iteration number. Since\n",
    "    Frank-Wolfe uses linear combinations of $s^{(t+1)}$ and $x^{(t)}$, it\n",
    "    tends to \"bounce around\" a lot, especially in earlier iterations.\n",
    "  </span>\n",
    "</div>\n",
    "\n",
    "<p><a id=\"proof\"></a></p>\n",
    "<h1><a name=\"proof\" href=\"#proof\">Why does it work?</a></h1>\n",
    "<p>We begin by making the two assumptions given earlier,</p>\n",
    "<ol>\n",
    "<li><mathjax>$f$</mathjax> is convex, differentiable, and finite for all <mathjax>$x \\in D$</mathjax></li>\n",
    "<li><mathjax>$D$</mathjax> is compact</li>\n",
    "</ol>\n",
    "<p><strong>Assumptions</strong> First, notice that we never needed to assume that a solution\n",
    "<mathjax>$x^{*}$</mathjax> exists. This is because <mathjax>$D$</mathjax> is compact and <mathjax>$f$</mathjax> is finite, meaning <mathjax>$x$</mathjax>\n",
    "cannot get bigger and bigger to make <mathjax>$f(x)$</mathjax> arbitrarily small.</p>\n",
    "<p>Secondly, we never made a Lipschitz assumption on <mathjax>$f$</mathjax> or its gradient. Since\n",
    "<mathjax>$D$</mathjax> is compact, we don't have to -- instead, we get the following for free.\n",
    "Define <mathjax>$C_f$</mathjax> as,</p>\n",
    "<p><mathjax>$$\n",
    "  C_f = \\max_{\\substack{\n",
    "                x,s \\in D \\\\\n",
    "                \\alpha \\in [0,1] \\\\\n",
    "                y = x + \\alpha (s-x)\n",
    "              }}\n",
    "          \\frac{2}{\\alpha^2} \\left(\n",
    "            f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle\n",
    "          \\right)\n",
    "$$</mathjax></p>\n",
    "<p>This immediate implies the following upper bound on <mathjax>$f$</mathjax> for all <mathjax>$x, y \\in\n",
    "D$</mathjax> and <mathjax>$\\alpha \\in [0,1]$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  f(y) \\le f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\alpha^2}{2} C_f\n",
    "$$</mathjax></p>\n",
    "<p><strong>Proof Outline</strong> The proof for Frank-Wolfe is surprisingly simple. The idea\n",
    "is to first upper bound <mathjax>$f(x^{(t+1)})$</mathjax> in terms of <mathjax>$f(x^{(t)})$</mathjax>, <mathjax>$g(x^{(t)})$</mathjax>,\n",
    "and <mathjax>$C_f$</mathjax>. We then transform this per-iteration bound into a bound on\n",
    "<mathjax>$f(x^{(t)}) - f(x^{*})$</mathjax> depending on <mathjax>$t$</mathjax> using induction. That's it!</p>\n",
    "<p><strong>Step 1</strong> Upper bound <mathjax>$f(x^{(t+1)})$</mathjax>. As usual, we'll denote <mathjax>$x^{+} \\triangleq\n",
    "x^{(t+1)}$</mathjax>, <mathjax>$x \\triangleq x^{(t)}$</mathjax>, <mathjax>$s^{+} \\triangleq s^{(t+1)}$</mathjax>, and <mathjax>$\\alpha\n",
    "\\triangleq \\alpha^{(t)}$</mathjax>. We begin by using the upper bound we just obtained for\n",
    "<mathjax>$f$</mathjax> in terms of <mathjax>$C_f$</mathjax>, substituting <mathjax>$x^{+} = (1 - \\alpha) x + \\alpha s^{+}$</mathjax> and\n",
    "then <mathjax>$g(x) = \\nabla f(x)^T (x - s^{+})$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{+}) \n",
    "  &amp; \\le f(x) + \\nabla f(x)^T (x^{+} - x) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; = f(x) + \\nabla f(x)^T ( (1-\\alpha) x + \\alpha s^{+} - x ) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; = f(x) + \\nabla f(x)^T ( \\alpha s^{+} - \\alpha x ) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; = f(x) - \\alpha \\nabla f(x)^T ( x - s^{+} ) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; = f(x) - \\alpha g(x) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p><strong>Step 2</strong> Use induction on <mathjax>$t$</mathjax>. First, recall the upper bound on <mathjax>$f(x) -\n",
    "f(x^{*}) \\le g(x)$</mathjax> <a href=\"#upper_bound\">we derived above</a>. Let's add <mathjax>$-f(x^{*})$</mathjax> into\n",
    "what we got from Step 1, then use the upper bound on <mathjax>$f(x) - f(x^{*})$</mathjax> to get,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{+}) - f(x^{*})\n",
    "  &amp; \\le f(x) - f(x^{*}) - \\alpha g(x) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; \\le f(x) - f(x^{*}) - \\alpha ( f(x) - f(x^{*}) ) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; = (1 - \\alpha) (f(x) - f(x^{*})) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Now, we employ induction on <mathjax>$t$</mathjax> to show that,</p>\n",
    "<p><mathjax>$$\n",
    "  f(x^{(t)}) - f(x^{*}) \\le \\frac{4 C_f / 2}{t+2}\n",
    "$$</mathjax></p>\n",
    "<p>We'll assume that the step size is <mathjax>$\\alpha^{(t)} = \\frac{2}{t+2}$</mathjax>, giving us\n",
    "<mathjax>$\\alpha^{(0)} = 2 / (0+2) = 1$</mathjax> and the base case,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{(1)} - f(x^{*})\n",
    "  &amp; \\le (1 - \\alpha^{(0)}) ( f(x^{(0)}) - f(x^{*}) ) + \\frac{\\alpha^2}{2} C_f \\\\\n",
    "  &amp; = (1 - 1) ( f(x^{(0)}) - f(x^{*}) ) + \\frac{1}{2} C_f \\\\\n",
    "  &amp; \\le \\frac{4 C_f / 2}{(0 + 1) + 2}\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Next, for the recursive case, we use the inductive assumption on <mathjax>$f(x) - f(x^{*})$</mathjax>, the definition of <mathjax>$\\alpha^{(t)}$</mathjax>, and some algebra,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  f(x^{+}) - f(x^{*})\n",
    "  &amp; \\le (1 - \\alpha) ( f(x) - f(x^{*}) ) + \\frac{ \\alpha^2}{2} C_f \\\\\n",
    "  &amp; \\le \\left(1 - \\frac{2}{t+2} \\right) \\frac{4 C_f / 2}{t + 2} + \\left( \\frac{2}{t+2} \\right)^2 C_f / 2 \\\\\n",
    "  &amp; \\le \\frac{4 C_f / 2}{t + 2} \\left( 1 - \\frac{2}{t+2} + \\frac{1}{t+2} \\right) \\\\\n",
    "  &amp; = \\frac{4 C_f / 2}{t + 2} \\left( \\frac{t+1}{t+2} \\right) \\\\\n",
    "  &amp; \\le \\frac{4 C_f / 2}{t + 2} \\left( \\frac{t+2}{t+3} \\right) \\\\\n",
    "  &amp; = \\frac{4 C_f / 2}{(t + 1) + 2} \\\\\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>Thus, if we want an error tolerance of <mathjax>$\\epsilon$</mathjax>, we need\n",
    "<mathjax>$O(\\frac{1}{\\epsilon})$</mathjax> iterations to find it. This matches the convergence\n",
    "rate of Gradient Descent an Proximal Gradient Descent, but falls short of their\n",
    "accelerated brethren.</p>\n",
    "<h1><a name=\"usage\" href=\"#usage\">When should I use it?</a></h1>\n",
    "<p>Like Proximal Gradient, efficient use of Frank-Wolfe requires solving a\n",
    "mini-optimization problem at each iteration. Unlike Proximal Gradient, however,\n",
    "this mini-problem will lead to unbounded iterates if the input space is not\n",
    "compact -- in other words, Frank-Wolfe cannot directly be applied when your\n",
    "domain is all of <mathjax>$R^{n}$</mathjax>. However, there is a very special case wherein\n",
    "Frank-Wolfe shines.</p>\n",
    "<p><a id=\"sparsity\"></a>\n",
    "  <strong>Sparsity</strong> The primary reason machine learning researchers have recently\n",
    "taken an interest in Frank-Wolfe is because in certain problems the iterates\n",
    "<mathjax>$x^{(t)}$</mathjax> will be extremely sparse.  Suppose that <mathjax>$D$</mathjax> is a polyhedron defined\n",
    "by a set of linear constraints. Then <mathjax>$s^{(t)}$</mathjax> is a solution to a Linear\n",
    "Program, meaning that each <mathjax>$s^{(t)}$</mathjax> lies on one of the vertices of the\n",
    "polyhedron. If these vertices have only a few non-zero entries, then <mathjax>$x^{(t)}$</mathjax>\n",
    "will too, as <mathjax>$x^{(t)}$</mathjax> is a linear combination of <mathjax>$s^{(1)} \\ldots s^{(t)}$</mathjax>.\n",
    "This is in direct contrast to gradient and proximal based methods, wherein\n",
    "<mathjax>$x^{(t)}$</mathjax> is the linear combination of a set of non-sparse <em>gradients</em>.</p>\n",
    "<p><strong>Atomic Norms</strong> One particular case where Frank-Wolfe shines is when\n",
    "minimizing <mathjax>$f(x)$</mathjax> subject to <mathjax>$||x|| \\le c$</mathjax> where <mathjax>$|| \\cdot ||$</mathjax> is an \"atomic\n",
    "norm\". We say that <mathjax>$||\\cdot||$</mathjax> is an atomic norm if <mathjax>$||x||$</mathjax> is the smallest <mathjax>$t$</mathjax>\n",
    "such that <mathjax>$x/t$</mathjax> is in the convex hull of a finite set of points <mathjax>$\\mathcal{A}$</mathjax>,\n",
    "that is,</p>\n",
    "<p><mathjax>$$\n",
    "  ||x|| = \\inf \\{ t : x \\in t \\, \\text{Conv}(\\mathcal{A}) \\}\n",
    "$$</mathjax></p>\n",
    "<p>For example, <mathjax>$||x||_1$</mathjax> is an atomic norm with <mathjax>$\\mathcal{A}$</mathjax> being the set of\n",
    "all vectors with only one <mathjax>$+1$</mathjax> or one <mathjax>$-1$</mathjax> entry. In these cases, finding\n",
    "<mathjax>$\\arg\\min_{||s|| \\le c} \\langle \\nabla f(x), s \\rangle$</mathjax> is tantamount to\n",
    "finding which element of <mathjax>$\\mathcal{A}$</mathjax> minimizes <mathjax>$\\langle \\nabla f(x), s\n",
    "\\rangle$</mathjax> (since <mathjax>$\\text{Conv}(\\mathcal{A})$</mathjax> defines a polyhedron). For a whole\n",
    "lot more on Atomic Norms, see <a href=\"http://pages.cs.wisc.edu/~brecht/papers/2010-crpw_inverse_problems.pdf\">this tome</a> by\n",
    "Chandrasekaranm et al.</p>\n",
    "<h1><a name=\"extensions\" href=\"#extensions\">Extensions</a></h1>\n",
    "<p><strong>Step Size</strong> The proof above relied on a step size of <mathjax>$\\alpha^{(t)} =\n",
    "\\frac{2}{t+2}$</mathjax>, but as usual <a href=\"/blog/gradient-descent.html#line_search\">Line Search</a> can be applied to\n",
    "accelerate convergence.</p>\n",
    "<p><strong>Approximate Linear Solutions</strong> Though not stated in the proof above,\n",
    "another cool point about Frank-Wolfe is that you don't actually need to solve\n",
    "the linear mini-problem exactly, but you will still converge to the optimal\n",
    "solution (albet at a slightly slower rate). In particular, assume that each\n",
    "mini-problem can be solved approximately with additive error <mathjax>$\\frac{\\delta\n",
    "C_f}{t+2}$</mathjax> at iteration <mathjax>$t$</mathjax>,</p>\n",
    "<p><mathjax>$$\n",
    "  \\langle s^{(t+1)}, \\nabla f(x^{(t)}) \\rangle\n",
    "  \\le \\min_{s} \\langle s, \\nabla f(x^{(t)}) \\rangle + \\frac{\\delta C_f}{t+2}\n",
    "$$</mathjax></p>\n",
    "<p>then Frank-Wolfe's rate of convergence is</p>\n",
    "<p><mathjax>$$\n",
    "  f(x^{(t)}) - f(x^{*}) \\le \\frac{2 C_f}{t+2} (1 + \\delta)\n",
    "$$</mathjax></p>\n",
    "<p>The proof for this can be found in the supplement to <a href=\"http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf\">Jaggi's</a>\n",
    "excellent survey on Frank-Wolfe for machine learning.</p>\n",
    "<h1><a name=\"invariance\" href=\"#invariance\">Linear Invariance</a></h1>\n",
    "<p>Another cool fact about Frank-Wolfe is that it's <em>linearly invariant</em> -- that\n",
    "is, if you rotate and scale the space, nothing changes about the convergence\n",
    "rate. This is in direct contrast to many other methods which depend on the\n",
    "<a href=\"http://en.wikipedia.org/wiki/Condition_number\">condition number</a> of a function (for functions with\n",
    "Hessians, this is the ratio between the largest and smallest eigenvalues,\n",
    "<mathjax>$\\sigma_{\\max} / \\sigma_{\\min})$</mathjax>.</p>\n",
    "<p>Suppose we transform our input space with a surjective (that is, onto) linear\n",
    "transformation <mathjax>$M: \\hat{D} \\rightarrow D$</mathjax>. Let's now try to solve the problem,</p>\n",
    "<p><mathjax>$$\n",
    "  \\min_{\\hat{x} \\in \\hat{D}} \\hat{f}(\\hat{x}) = f(M \\hat{x}) = f(x)\n",
    "$$</mathjax></p>\n",
    "<p>Let's look at the solution to the per-iteration mini-problem we need to solve\n",
    "for Frank-Wolfe,</p>\n",
    "<p><mathjax>$$\n",
    "\\begin{align*}\n",
    "  \\min_{\\hat{s} \\in \\hat{D}} \\langle \\nabla \\hat{f}(\\hat{x}), \\hat{s} \\rangle\n",
    "  = \\min_{\\hat{s} \\in \\hat{D}} \\langle M^T \\nabla f( M \\hat{x}), \\hat{s} \\rangle\n",
    "  = \\min_{\\hat{s} \\in \\hat{D}} \\langle \\nabla f( x ), M \\hat{s} \\rangle\n",
    "  = \\min_{s \\in D} \\langle \\nabla f( x ), s \\rangle\n",
    "\\end{align*}\n",
    "$$</mathjax></p>\n",
    "<p>In other words, we will find the same <mathjax>$s$</mathjax> if we solve in the original space,\n",
    "or if we find <mathjax>$\\hat{s}$</mathjax> and then map it back to <mathjax>$s$</mathjax>. No matter how <mathjax>$M$</mathjax> warps\n",
    "the space, Frank-Wolfe will do the same thing. This also means that if there's\n",
    "a linear transformation you can do to make the points of your polyhedron\n",
    "sparse, you can do it with no penalty!</p>\n",
    "<h1><a name=\"references\" href=\"#references\">References</a></h1>\n",
    "<p><strong>Proof of Convergence, Linear Invariance</strong> Pretty much everything in this\n",
    "article comes from <a href=\"http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf\">Jaggi's</a> fantastic article on Frank-Wolfe for\n",
    "machine learning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frank_wolfe(minisolver, gradient, alpha, x0, epsilon=1e-2):\n",
    "  \"\"\"Frank-Wolfe Algorithm\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  minisolver : function\n",
    "      minisolver(x) = argmin_{s \\in D} <x, s>\n",
    "  gradient : function\n",
    "      gradient(x) = gradient[f](x)\n",
    "  alpha : function\n",
    "      learning rate\n",
    "  x0 : array\n",
    "      initial value for x\n",
    "  epsilon : float\n",
    "      desired accuracy\n",
    "  \"\"\"\n",
    "  xs = [x0]\n",
    "  iteration = 0\n",
    "  while True:\n",
    "    x = xs[-1]\n",
    "    g = gradient(x)\n",
    "    s_next = minisolver(g)\n",
    "    if g * (x - s_next) <= epsilon:\n",
    "      break\n",
    "    a = alpha(iteration=iteration, x=x, direction=s_next)\n",
    "    x_next = (1 - a) * x + a * s_next\n",
    "    xs.append(x_next)\n",
    "    iteration += 1\n",
    "  return xs\n",
    "\n",
    "\n",
    "def default_learning_rate(iteration, **kwargs):\n",
    "  return 2.0 / (iteration + 2.0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  import os\n",
    "\n",
    "  import numpy as np\n",
    "  import pylab as pl\n",
    "  import yannopt.plotting as plotting\n",
    "\n",
    "  ### FRANK WOLFE ALGORITHM ###\n",
    "\n",
    "  # problem definition\n",
    "  function    = lambda x: (x - 0.5) ** 2 + 2 * x\n",
    "  gradient    = lambda x: 2 * (x - 0.5) + 2\n",
    "  minisolver  = lambda y: -1 if y > 0 else 2 # D = [-1, 2]\n",
    "  x0 = 1.0\n",
    "\n",
    "  # run gradient descent\n",
    "  iterates = frank_wolfe(minisolver, gradient, default_learning_rate, x0)\n",
    "\n",
    "  ### PLOTTING ###\n",
    "\n",
    "  plotting.plot_iterates_vs_function(iterates, function,\n",
    "                                     path='figures/iterates.png', y_star=0.0)\n",
    "  plotting.plot_iteration_vs_function(iterates, function,\n",
    "                                      path='figures/convergence.png', y_star=0.0)\n",
    "\n",
    "  # make animation\n",
    "  iterates = np.asarray(iterates)\n",
    "  try:\n",
    "    os.makedirs('figures/animation')\n",
    "  except OSError:\n",
    "    pass\n",
    "\n",
    "  for t in range(len(iterates)-1):\n",
    "    x = iterates[t]\n",
    "    x_plus = iterates[t+1]\n",
    "    s_plus = minisolver(gradient(x))\n",
    "\n",
    "    f = function\n",
    "    g = gradient\n",
    "    f_hat = lambda y: f(x) + g(x) * (y - x)\n",
    "\n",
    "    xmin, xmax = plotting.limits([-1, 2])\n",
    "    ymin, ymax = -4, 8\n",
    "\n",
    "    pl.plot(np.linspace(xmin ,xmax), function(np.linspace(xmin, xmax)), alpha=0.2)\n",
    "    pl.xlim([xmin, xmax])\n",
    "    pl.ylim([ymin, ymax])\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('f(x)')\n",
    "\n",
    "    pl.plot([xmin, xmax], [f_hat(xmin), f_hat(xmax)], '--', alpha=0.2)\n",
    "    pl.vlines([-1, 2], ymin, ymax, color=np.ones((2,3)) * 0.2, linestyle='solid')\n",
    "    pl.scatter(x, f(x), c=[0.8, 0.0, 0.0], marker='o', alpha=0.8)\n",
    "    pl.scatter(x_plus, f(x_plus), c=[0.0, 0.8, 0.0], marker='D', alpha=0.8)\n",
    "    pl.vlines(x_plus, f_hat(x_plus), f(x_plus), color=[0.0,0.8,0.0], linestyle='dotted')\n",
    "    pl.scatter(s_plus, f_hat(s_plus), c=0.35, marker='x', alpha=0.8)\n",
    "\n",
    "    pl.savefig('figures/animation/%02d.png' % t)\n",
    "    pl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
